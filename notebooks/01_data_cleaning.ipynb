{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e413d4d",
   "metadata": {},
   "source": [
    "# Data cleaning & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cf3e",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c7d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38c32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9bee6b",
   "metadata": {},
   "source": [
    "## 1. Pre-cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606f331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and standardize case references\n",
    "datasets = {\n",
    "    'referrals': pd.read_csv(\"../data/raw/BRC-Data/Cases_depersonalised.csv\"),\n",
    "    'hiu': pd.read_csv(\"../data/raw/BRC-Data/HIU_depersonalised.csv\"),\n",
    "    'snap': pd.read_csv(\"../data/raw/BRC-Data/SNAP_depersonalised.csv\")\n",
    "}\n",
    "\n",
    "# Standardize case reference columns\n",
    "datasets['referrals']['case_ref'] = datasets['referrals']['Case Reference']\n",
    "datasets['hiu']['case_ref'] = 'CAS-' + datasets['hiu']['Q2.1. CAS-'].astype(str).str.replace('.0', '', regex=False)\n",
    "datasets['snap']['case_ref'] = datasets['snap']['BRM case number:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da0659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICATE ANALYSIS BEFORE CLEANING ===\n",
      "\n",
      "REFERRALS Dataset:\n",
      "  Total rows: 181,085\n",
      "  Unique case_ref: 126,717\n",
      "  Analysing duplicates based on: ['case_ref', 'Referral Notes (depersonalised)', 'Referral Date/Time']\n",
      "  Perfect duplicates (relevant columns): 52,146\n",
      "  Duplicate case_ref: 54,368\n",
      "  Top 5 duplicate case_ref by count:\n",
      "    CAS-537284: 60 rows\n",
      "    CAS-530729: 45 rows\n",
      "    CAS-565892: 42 rows\n",
      "    CAS-592629: 42 rows\n",
      "    CAS-537229: 40 rows\n",
      "    For case CAS-537284: 45/59 duplicates are identical (relevant data)\n",
      "    Non-identical relevant columns for CAS-537284:\n",
      "      Referral Notes (depersonalised): ['XXXX hazard', '4.Fall', '4.Trips and falls', '4.Trips and falls', 'XXXX hazard', '4.Fall', '4.Trips and falls', 'XXXX hazard', '4.Trips and falls', 'XXXX of benefits', '3.Person who client claimed kidnapped her is now o...', 'Distress', 'XXXX of benefits', 'XXXX hazard', '3.Person who client claimed kidnapped her is now o...', 'Distress', 'abusive behaviour', 'XXXX of benefits', '4.Fall', 'Threatened suicide', 'abusive behaviour', 'client beimg abusive to satff', '4.Alleged physical and financial', 'XXXX in health', 'Collapsed in flat', 'Deterioration innhealth', 'client beimg abusive to satff', 'XXXX in health', '4.Stroke', 'client PERSON lost while outdoors', 'Collapsed in flat', 'Deterioration innhealth', '3.Person who client claimed kidnapped her is now o...', '4.Alleged physical and financial', 'XXXX in health', 'Collapsed in flat', 'Deterioration innhealth', 'Distress', 'Threatened suicide', 'abusive behaviour', '4.Alleged physical and financial', 'XXXX in health', '4.Stroke', 'client PERSON lost while outdoors', 'Collapsed in flat', 'Deterioration innhealth', '4.Stroke', 'client PERSON lost while outdoors', 'Threatened suicide', 'abusive behaviour', '4.Alleged physical and financial', '4.Fall', 'Threatened suicide', 'client beimg abusive to satff', '4.Stroke', 'client PERSON lost while outdoors', 'client beimg abusive to satff', 'XXXX of benefits', '3.Person who client claimed kidnapped her is now o...', 'Distress']\n",
      "\n",
      "HIU Dataset:\n",
      "  Total rows: 2,854\n",
      "  Unique case_ref: 1,430\n",
      "  Analysing duplicates excluding 4 depersonalised columns\n",
      "  Perfect duplicates (relevant columns): 448\n",
      "  Duplicate case_ref: 1,424\n",
      "  Top 5 duplicate case_ref by count:\n",
      "    CAS-nan: 451 rows\n",
      "    CAS-565184: 4 rows\n",
      "    CAS-590900: 4 rows\n",
      "    CAS-602966: 4 rows\n",
      "    CAS-596122: 4 rows\n",
      "    For case CAS-nan: 448/450 duplicates are identical (relevant data)\n",
      "    Non-identical relevant columns for CAS-nan:\n",
      "      Q6. Why wasn't it possible to record outcomes for this client?: ['-', 'Admitted to hospital', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      Q9.1. Iâ€™ve been feeling optimistic about the future: [3.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      Q9.2. Iâ€™ve been feeling useful: [3.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      Q9.3. Iâ€™ve been feeling relaxed: [3.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      Q9.4. Iâ€™ve been dealing with problems well: [2.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      Q9.5. Iâ€™ve been thinking clearly: [2.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      Q9.6. Iâ€™ve been feeling close to other people: [2.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      Q9.7. Iâ€™ve been able to make up my own mind about things: [3.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      Q10. Which of the following best describe your current home/housing situation?: [1.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      Q11. What best describes your current financial situation?: [2.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      SWEMWEBS Score: [18.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "      SWEWBS Transformed Score: ['17.43', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING', 'MISSING']\n",
      "      Profile: ['Not recordedLow WellbeingHousing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need', 'Not recorded Housing NeedFinance Need']\n",
      "\n",
      "SNAP Dataset:\n",
      "  Total rows: 2,012\n",
      "  Unique case_ref: 1,305\n",
      "  Analysing duplicates excluding 7 depersonalised columns\n",
      "  Perfect duplicates (relevant columns): 54\n",
      "  Duplicate case_ref: 706\n",
      "  Top 5 duplicate case_ref by count:\n",
      "    CAS-606643: 2 rows\n",
      "    CAS-589128: 2 rows\n",
      "    CAS-560176: 2 rows\n",
      "    CAS-560343: 2 rows\n",
      "    CAS-563029: 2 rows\n",
      "    For case CAS-606643: 0/1 duplicates are identical (relevant data)\n",
      "    Non-identical relevant columns for CAS-606643:\n",
      "      Survey completed:: ['at the start of support', 'at the end of support']\n",
      "      Date of assessment: ['04/12/2024', '16/12/2024']\n",
      "      Timepoint: [1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DUPLICATE ANALYSIS BEFORE CLEANING ===\")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()} Dataset:\")\n",
    "    print(f\"  Total rows: {len(df):,}\")\n",
    "    print(f\"  Unique case_ref: {df['case_ref'].nunique():,}\")\n",
    "    \n",
    "    # Define columns to exclude from duplication analysis (depersonalized/randomized)\n",
    "    if name == 'referrals':\n",
    "        # For referrals: only consider case_ref and referral notes\n",
    "        analysis_cols = ['case_ref', 'Referral Notes (depersonalised)', 'Referral Date/Time']\n",
    "        available_cols = [col for col in analysis_cols if col in df.columns]\n",
    "        print(f\"  Analysing duplicates based on: {available_cols}\")\n",
    "    elif name == 'snap':\n",
    "        # Exclude: Has Disability, IMD Decile, Country, Age, Gender, Ethnicity, Living Arrangements\n",
    "        exclude_cols = ['Has Disability', 'IMD Decile', 'Country', 'Age', 'Gender', 'Ethnicity', 'Living Arrangements']\n",
    "        available_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        print(f\"  Analysing duplicates excluding {len(exclude_cols)} depersonalised columns\")\n",
    "    elif name == 'hiu':\n",
    "        # Exclude: Age, Gender, Ethnicity, Living Arrangements\n",
    "        exclude_cols = ['Age', 'Gender', 'Ethnicity', 'Living Arrangements']\n",
    "        available_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        print(f\"  Analysing duplicates excluding {len(exclude_cols)} depersonalised columns\")\n",
    "    \n",
    "    # Check for perfect duplicates based on relevant columns only\n",
    "    perfect_duplicates = df.duplicated(subset=available_cols).sum()\n",
    "    print(f\"  Perfect duplicates (relevant columns): {perfect_duplicates:,}\")\n",
    "    \n",
    "    # Check for duplicates by case_ref only\n",
    "    case_ref_duplicates = df['case_ref'].duplicated().sum()\n",
    "    print(f\"  Duplicate case_ref: {case_ref_duplicates:,}\")\n",
    "    \n",
    "    if case_ref_duplicates > 0:\n",
    "        # Show examples of duplicate case_ref: Show top 5 by count\n",
    "        duplicate_cases = df[df['case_ref'].duplicated(keep=False)]['case_ref'].value_counts().head(5)\n",
    "        print(f\"  Top 5 duplicate case_ref by count:\")\n",
    "        for case_ref, count in duplicate_cases.items():\n",
    "            print(f\"    {case_ref}: {count} rows\")\n",
    "        \n",
    "        # Check if duplicate case_ref have identical relevant data - CHANGED: Use highest count case\n",
    "        highest_count_case = duplicate_cases.index[0]  # This is now the case with most duplicates\n",
    "        duplicate_rows = df[df['case_ref'] == highest_count_case]\n",
    "        \n",
    "        # Check duplicates based on relevant columns only\n",
    "        duplicate_subset = duplicate_rows[available_cols]\n",
    "        identical_duplicates = duplicate_subset.duplicated().sum()\n",
    "        \n",
    "        print(f\"    For case {highest_count_case}: {identical_duplicates}/{len(duplicate_rows)-1} duplicates are identical (relevant data)\")\n",
    "        \n",
    "        # Show what's different in duplicate rows (if any) - only relevant columns\n",
    "        if identical_duplicates < len(duplicate_rows) - 1:\n",
    "            print(f\"    Non-identical relevant columns for {highest_count_case}:\")\n",
    "            for col in available_cols:\n",
    "                if col in duplicate_rows.columns:\n",
    "                    unique_vals = duplicate_rows[col].nunique()\n",
    "                    if unique_vals > 1:\n",
    "                        values = duplicate_rows[col].tolist()\n",
    "                        # For referral notes, show preview\n",
    "                        if col == 'Referral Notes (depersonalised)':\n",
    "                            values = [str(v)[:50] + \"...\" if len(str(v)) > 50 else str(v) for v in values]\n",
    "                        print(f\"      {col}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbeb98f",
   "metadata": {},
   "source": [
    "### 1.1 Pre-cleaning referrals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e46c7b",
   "metadata": {},
   "source": [
    "I start with the pre-cleaning of referrals.\n",
    "\n",
    "1. I remove identical rows based on: ['case_ref', 'Referral Notes (depersonalised)', 'Referral Date/Time']. Those are pure duplicates (other columns might be different due to the depersonalisation).\n",
    "\n",
    "Then, for each CAS which has multiple rows (most likely on different dates):\n",
    "\n",
    "2. I create two columns: one with the number of observations, and one with the date range of those observations;\n",
    "3. Then, if one or some of the rows have a non-NA referral, I remove the rows which don't. \n",
    "4. Finally, if all observations have the same referrals, or if none of the observations have a referral, I only keep the most recent row (the columns created earlier will keep the other relevant information).\n",
    "\n",
    "The only duplicated CAS left will be those with different referrals on different dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b4b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REFERRALS DATASET PRE-CLEANING ===\n",
      "Initial dataset: 181,085 rows, 126,717 unique cases\n",
      "\n",
      "Step 0: Cleaning NA variations in text columns...\n",
      "\n",
      "Step 1: Removing identical rows...\n",
      "  Using columns: ['case_ref', 'Referral Notes (depersonalised)', 'Referral Date/Time']\n",
      "  Removed 52,146 identical rows\n",
      "  Remaining: 128,939 rows, 126,717 unique cases\n",
      "\n",
      "Step 2: Processing cases with multiple observations...\n",
      "  Step 2a: Adding observation count and date range columns...\n",
      "  Step 2b: Removing rows without referral notes when others exist...\n",
      "    Removed 0 rows without referral notes\n",
      "  Step 2c: Consolidating cases with identical or missing referral notes...\n",
      "    Consolidated 0 rows with identical/missing referral notes\n",
      "\n",
      "Final dataset: 128,939 rows, 126,717 unique cases\n",
      "\n",
      "Cases with multiple rows remaining (different referrals): 1,434\n",
      "Top 5 cases by number of remaining rows:\n",
      "  CAS-537284: 15 rows\n",
      "  CAS-530729: 9 rows\n",
      "  CAS-483850: 8 rows\n",
      "  CAS-503700: 8 rows\n",
      "  CAS-560840: 8 rows\n",
      "\n",
      "Example case CAS-537284 with different referrals:\n",
      "  2023-12-04: XXXX hazard...\n",
      "  2023-12-04: 4.Fall...\n",
      "  2023-12-04: 4.Trips and falls...\n",
      "  2023-12-04: XXXX of benefits...\n",
      "  2023-12-04: 3.Person who client claimed kidnapped her is now out on bail...\n",
      "  2023-12-04: Distress...\n",
      "  2023-12-04: abusive behaviour...\n",
      "  2023-12-04: Threatened suicide...\n",
      "  2023-12-04: client beimg abusive to satff...\n",
      "  2023-12-04: 4.Alleged physical and financial...\n",
      "  2023-12-04: XXXX in health...\n",
      "  2023-12-04: Collapsed in flat...\n",
      "  2023-12-04: Deterioration innhealth...\n",
      "  2023-12-04: 4.Stroke...\n",
      "  2023-12-04: client PERSON lost while outdoors...\n"
     ]
    }
   ],
   "source": [
    "from utils.data_cleaning_helpers import clean_referrals_dataset\n",
    "\n",
    "# Load the referrals data\n",
    "referrals_df = datasets['referrals'].copy()\n",
    "\n",
    "# Clean the dataset\n",
    "referrals_cleaned = clean_referrals_dataset(referrals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d8c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANALYZING REFERRALS BY DATE PATTERNS ===\n",
      "Cases with multiple referrals: 1,434\n",
      "Cases with single referral: 125,283\n",
      "\n",
      "=== DATE PATTERNS FOR 1,434 MULTI-REFERRAL CASES ===\n",
      "Cases where ALL referrals are on the SAME date: 1,434 (100.0%)\n",
      "Cases with referrals on MULTIPLE dates: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== ANALYSING REFERRALS BY DATE PATTERNS ===\")\n",
    "\n",
    "# Convert date column to datetime if not already\n",
    "referrals_cleaned['Referral Date/Time'] = pd.to_datetime(referrals_cleaned['Referral Date/Time'], errors='coerce')\n",
    "referrals_cleaned['referral_date'] = referrals_cleaned['Referral Date/Time'].dt.date\n",
    "\n",
    "# Cases with multiple referrals\n",
    "multi_referral_cases = referrals_cleaned['case_ref'].value_counts()\n",
    "multi_referral_cases = multi_referral_cases[multi_referral_cases > 1]\n",
    "\n",
    "print(f\"Cases with multiple referrals: {len(multi_referral_cases):,}\")\n",
    "print(f\"Cases with single referral: {referrals_cleaned['case_ref'].nunique() - len(multi_referral_cases):,}\")\n",
    "\n",
    "# Analyze date patterns for multi-referral cases\n",
    "print(f\"\\n=== DATE PATTERNS FOR {len(multi_referral_cases):,} MULTI-REFERRAL CASES ===\")\n",
    "\n",
    "same_date_stats = []\n",
    "for case_ref in multi_referral_cases.index:\n",
    "    case_data = referrals_cleaned[referrals_cleaned['case_ref'] == case_ref].copy()\n",
    "    \n",
    "    # Count unique dates for this case\n",
    "    unique_dates = case_data['referral_date'].nunique()\n",
    "    total_referrals = len(case_data)\n",
    "    \n",
    "    same_date_stats.append({\n",
    "        'case_ref': case_ref,\n",
    "        'total_referrals': total_referrals,\n",
    "        'unique_dates': unique_dates,\n",
    "        'all_same_date': unique_dates == 1,\n",
    "        'multiple_dates': unique_dates > 1\n",
    "    })\n",
    "\n",
    "same_date_df = pd.DataFrame(same_date_stats)\n",
    "\n",
    "# Summary statistics\n",
    "all_same_date = same_date_df['all_same_date'].sum()\n",
    "multiple_dates = same_date_df['multiple_dates'].sum()\n",
    "\n",
    "print(f\"Cases where ALL referrals are on the SAME date: {all_same_date:,} ({all_same_date/len(multi_referral_cases)*100:.1f}%)\")\n",
    "print(f\"Cases with referrals on MULTIPLE dates: {multiple_dates:,} ({multiple_dates/len(multi_referral_cases)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7dde9a",
   "metadata": {},
   "source": [
    "For cases which have multiple observations with DIFFERENT referrals, they are always done on the same date. I decide to keep the longest referral, to keep the maximum information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e082643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONSOLIDATING REFERRALS TO ONE PER CASE ===\n",
      "Initial referrals: 128,939 rows, 126,717 unique cases\n",
      "Cases with multiple referrals: 1,434\n",
      "After consolidation: 126,717 rows (one per case)\n",
      "\n",
      "Consolidated referrals saved to: ../data/processed/referrals_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "from utils.data_cleaning_helpers import consolidate_referrals_longest\n",
    "\n",
    "# Apply consolidation\n",
    "referrals_consolidated = consolidate_referrals_longest(referrals_cleaned)\n",
    "\n",
    "# Save consolidated dataset\n",
    "referrals_consolidated.to_csv(\"../data/processed/referrals_cleaned.csv\", index=False)\n",
    "print(f\"\\nConsolidated referrals saved to: ../data/processed/referrals_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81666df",
   "metadata": {},
   "source": [
    "### 1.2 Pre-cleaning SNAP data & merging with referrals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01717e",
   "metadata": {},
   "source": [
    "The pre-cleaning of SNAP is done as follows:\n",
    "\n",
    "1. I removed perfect duplicates based on columns that were not randomised in the depersonalisation process.\n",
    "\n",
    "Then, I noticed that CAS have either one or two observations (not more). These correspond to observations at the start and / or at the end of support from the BRC. I get a brief overview.\n",
    "\n",
    "2. For each case, I determine what type of valid assessments are available, only counting valid where 'Possible to record outcomes:' == 'Yes'. \n",
    "3. I then create a summary for each case showing:\n",
    "    - Total number of assessments\n",
    "    - Whether valid baseline assessment exists (timepoint 1.0 + recordable outcomes)\n",
    "    - Whether valid post-support assessment exists (timepoint 2.0 + recordable outcomes)\n",
    "    - Whether case has both valid assessments (complete usable pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85b26266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SNAP DATASET PRE-CLEANING ===\n",
      "Initial dataset: 2,012 rows, 1,305 unique cases\n",
      "\n",
      "Step 0: Cleaning NA variations in text columns...\n",
      "\n",
      "Step 1: Removing perfect duplicates...\n",
      "  Removed 54 perfect duplicates\n",
      "\n",
      "Step 2: Identifying valid baseline and valid post-support assessments...\n",
      "  Cases with valid baseline: 983\n",
      "  Cases with valid baseline only: 508\n",
      "  Cases with valid post-support: 540\n",
      "\n",
      "  Cases with both (complete valid pairs): 475\n",
      "\n",
      "Step 3: Adding metadata columns...\n",
      "\n",
      "Final dataset: 1,958 rows, 1,305 unique cases\n",
      "\n",
      "Cleaned dataset saved to: ../data/processed/snap_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "from utils.data_cleaning_helpers import clean_snap_dataset\n",
    "\n",
    "snap_df = datasets['snap'].copy()\n",
    "\n",
    "# Clean the SNAP dataset\n",
    "snap_cleaned = clean_snap_dataset(snap_df)\n",
    "\n",
    "# Save cleaned dataset\n",
    "snap_cleaned.to_csv(\"../data/processed/snap_cleaned.csv\", index=False)\n",
    "print(f\"\\nCleaned dataset saved to: ../data/processed/snap_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbec9b1",
   "metadata": {},
   "source": [
    "**Merging with referrals**\n",
    "\n",
    "I now proceed to creating a merged dataset with referrals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f50245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MERGING REFERRALS WITH SNAP DATA ===\n",
      "Referrals dataset: 126,717 rows, 126,717 unique cases\n",
      "SNAP dataset: 1,958 rows, 1,305 unique cases\n",
      "\n",
      "Cases in both datasets: 1,222\n",
      "Cases only in referrals: 125,495\n",
      "Cases only in SNAP: 84\n",
      "\n",
      "After filtering to common cases:\n",
      "Referrals: 1,222 rows\n",
      "SNAP: 1,856 rows\n",
      "\n",
      "Merged dataset: 1,856 rows, 1,222 unique cases\n",
      "\n",
      "Rows per case in merged dataset:\n",
      "  1 row (baseline only): 588 cases\n",
      "  2 rows (baseline + outcome): 634 cases\n",
      "\n",
      "SNAP assessment validity in merged dataset:\n",
      "  Cases with valid baseline + outcome: 472\n",
      "  Cases with valid baseline only: 471\n",
      "\n",
      "Merged dataset saved to: ../data/processed/merged_referrals_snap.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_referrals_with_snap():\n",
    "    \"\"\"\n",
    "    Merge referrals (1 row per case) with SNAP data (1-2 rows per case).\n",
    "    Only keeps cases that appear in both datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== MERGING REFERRALS WITH SNAP DATA ===\")\n",
    "    \n",
    "    # Load datasets\n",
    "    referrals_df = pd.read_csv(\"../data/processed/referrals_cleaned.csv\")\n",
    "    snap_df = pd.read_csv(\"../data/processed/snap_cleaned.csv\")\n",
    "    \n",
    "    print(f\"Referrals dataset: {len(referrals_df):,} rows, {referrals_df['case_ref'].nunique():,} unique cases\")\n",
    "    print(f\"SNAP dataset: {len(snap_df):,} rows, {snap_df['case_ref'].nunique():,} unique cases\")\n",
    "    \n",
    "    # Find cases that appear in both datasets\n",
    "    referrals_cases = set(referrals_df['case_ref'])\n",
    "    snap_cases = set(snap_df['case_ref'])\n",
    "    \n",
    "    common_cases = referrals_cases.intersection(snap_cases)\n",
    "    \n",
    "    print(f\"\\nCases in both datasets: {len(common_cases):,}\")\n",
    "    print(f\"Cases only in referrals: {len(referrals_cases - snap_cases):,}\")\n",
    "    print(f\"Cases only in SNAP: {len(snap_cases - referrals_cases):,}\")\n",
    "    \n",
    "    # Filter both datasets to common cases only\n",
    "    referrals_common = referrals_df[referrals_df['case_ref'].isin(common_cases)].copy()\n",
    "    snap_common = snap_df[snap_df['case_ref'].isin(common_cases)].copy()\n",
    "    \n",
    "    print(f\"\\nAfter filtering to common cases:\")\n",
    "    print(f\"Referrals: {len(referrals_common):,} rows\")\n",
    "    print(f\"SNAP: {len(snap_common):,} rows\")\n",
    "    \n",
    "    # Merge: each referral row will be duplicated for each SNAP row of the same case\n",
    "    merged_df = snap_common.merge(referrals_common, on='case_ref', how='inner', suffixes=('_snap', '_referral'))\n",
    "    \n",
    "    print(f\"\\nMerged dataset: {len(merged_df):,} rows, {merged_df['case_ref'].nunique():,} unique cases\")\n",
    "    \n",
    "    # Show structure breakdown\n",
    "    rows_per_case = merged_df['case_ref'].value_counts()\n",
    "    print(f\"\\nRows per case in merged dataset:\")\n",
    "    print(f\"  1 row (baseline only): {(rows_per_case == 1).sum():,} cases\")\n",
    "    print(f\"  2 rows (baseline + outcome): {(rows_per_case == 2).sum():,} cases\")\n",
    "    if (rows_per_case > 2).any():\n",
    "        print(f\"  >2 rows (unexpected): {(rows_per_case > 2).sum():,} cases\")\n",
    "    \n",
    "    # Show SNAP assessment validity breakdown for merged cases\n",
    "    if 'has_both' in merged_df.columns:\n",
    "        cases_with_both = merged_df.drop_duplicates('case_ref')['has_both'].sum()\n",
    "        cases_baseline_only = merged_df.drop_duplicates('case_ref')['has_valid_baseline'].sum() - cases_with_both\n",
    "        \n",
    "        print(f\"\\nSNAP assessment validity in merged dataset:\")\n",
    "        print(f\"  Cases with valid baseline + outcome: {cases_with_both:,}\")\n",
    "        print(f\"  Cases with valid baseline only: {cases_baseline_only:,}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Perform the merge\n",
    "merged_referrals_snap = merge_referrals_with_snap()\n",
    "\n",
    "# Save merged dataset\n",
    "merged_referrals_snap.to_csv(\"../data/processed/merged_referrals_snap.csv\", index=False)\n",
    "print(f\"\\nMerged dataset saved to: ../data/processed/merged_referrals_snap.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc008f",
   "metadata": {},
   "source": [
    "### 1.3 Pre-cleaning HIU data & merging with referrals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde67a2d",
   "metadata": {},
   "source": [
    "## 2. Merging data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f037fe",
   "metadata": {},
   "source": [
    "**Merging strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bcee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset overlap summary\n",
    "case_sets = {name: set(df['case_ref'].dropna()) for name, df in datasets.items()}\n",
    "print(f\"Dataset sizes: Referrals: {len(case_sets['referrals']):,}, HIU: {len(case_sets['hiu']):,}, SNAP: {len(case_sets['snap']):,}\")\n",
    "print(f\"Overlaps: Ref âˆ© HIU: {len(case_sets['referrals'] & case_sets['hiu']):,}, Ref âˆ© SNAP: {len(case_sets['referrals'] & case_sets['snap']):,}, HIU âˆ© SNAP: {len(case_sets['hiu'] & case_sets['snap']):,}, All: {len(case_sets['referrals'] & case_sets['hiu'] & case_sets['snap']):,}\")\n",
    "\n",
    "# Add prefixes to HIU and SNAP columns (except case_ref)\n",
    "hiu_renamed = datasets['hiu'].rename(columns={col: f'hiu_{col}' for col in datasets['hiu'].columns if col != 'case_ref'})\n",
    "snap_renamed = datasets['snap'].rename(columns={col: f'snap_{col}' for col in datasets['snap'].columns if col != 'case_ref'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4cc9f",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14ad3780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2588692/1158086254.py:4: DtypeWarning: Columns (22,42,43,44,45,48,50,51,52,53,54,55,64,66,67,68,69,70,71,73,77,78,79,80,81,82,83,84,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,134,163,164,165,166,167,168,169,170,171,173,175,176,177) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged = pd.read_csv(\"../data/processed/BRC_referrals_merged.csv\")\n"
     ]
    }
   ],
   "source": [
    "from utils.data_cleaning_helpers import get_note_length_category\n",
    "\n",
    "# Import merged dataset\n",
    "merged = pd.read_csv(\"../data/processed/BRC_referrals_merged.csv\")\n",
    "\n",
    "# Add note categories\n",
    "merged['note_length_category'] = merged['Referral Notes (depersonalised)'].apply(get_note_length_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70f126dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REFERRAL NOTES QUALITY ===\n",
      "Short note (<5 words): 63,549 (34.5%)\n",
      "Medium note (5-19 words): 46,007 (25.0%)\n",
      "Long note (20+ words): 43,475 (23.6%)\n",
      "No note: 31,019 (16.9%)\n"
     ]
    }
   ],
   "source": [
    "note_counts = merged['note_length_category'].value_counts()\n",
    "\n",
    "print(f\"\\n=== REFERRAL NOTES QUALITY ===\")\n",
    "for category, count in note_counts.items():\n",
    "    print(f\"{category}: {count:,} ({count/len(merged):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc6757",
   "metadata": {},
   "source": [
    "Let's first look at the characteristics of referrals for cases which have health outcomes data (in SNAP and HIE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d2568",
   "metadata": {},
   "source": [
    "### 3.1. SNAP (Support at Home, Care at Home, Hospital at Home, and Social Prescribing services)\n",
    "\n",
    "These services have started using an outcomes framework called SNAP (Social Needs and\n",
    "Preferences), which was only introduced in late 2024.\n",
    "\n",
    "I consider 'valid' SNAP data when the service user has responded to the survey (i.e. 'Possible to record outcomes:' == Yes). This does not necessarily mean all questions were answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c955a8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SNAP DATA OVERVIEW ===\n",
      "Total SNAP observations: 3,468\n",
      "Unique cases: 1,222\n",
      "Date range: 01/05/2024 to 31/10/2024\n",
      "\n",
      "Valid SNAP data: 2,839 observations (81.9%)\n",
      "Valid unique cases: 1,006\n",
      "\n",
      "=== ASSESSMENT TIMEPOINTS ===\n",
      "at the start of support: 2,050 (59.1%)\n",
      "at the end of support: 1,418 (40.9%)\n",
      "\n",
      "=== BASELINE + OUTCOMES AVAILABILITY ===\n",
      "Total unique cases with valid data: 1,006\n",
      "\n",
      "Cases with valid baseline only: 471\n",
      "Cases with valid outcomes only: 63\n",
      "\n",
      "Cases with both valid baseline + outcomes: 472\n",
      "Cases with both timepoints (any validity): 621\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to only include referrals with SNAP data\n",
    "merged_snap = merged[merged['has_snap']].copy()\n",
    "merged_snap = merged_snap.loc[:, ~merged_snap.columns.str.startswith('hiu_')]\n",
    "\n",
    "cols_snap = list(merged_snap.columns)\n",
    "\n",
    "print(\"=== SNAP DATA OVERVIEW ===\")\n",
    "print(f\"Total SNAP observations: {len(merged_snap):,}\")\n",
    "print(f\"Unique cases: {merged_snap['case_ref'].nunique():,}\")\n",
    "print(f\"Date range: {merged_snap['snap_Date of assessment'].min()} to {merged_snap['snap_Date of assessment'].max()}\")\n",
    "\n",
    "# Check for valid SNAP data\n",
    "valid_snap = merged_snap['snap_Possible to record outcomes:'] == 'Yes'\n",
    "print(f\"\\nValid SNAP data: {valid_snap.sum():,} observations ({valid_snap.mean():.1%})\")\n",
    "print(f\"Valid unique cases: {merged_snap[valid_snap]['case_ref'].nunique():,}\")\n",
    "\n",
    "# Assessment timepoints\n",
    "timepoint_counts = merged_snap['snap_Survey completed:'].value_counts()\n",
    "print(f\"\\n=== ASSESSMENT TIMEPOINTS ===\")\n",
    "for timepoint, count in timepoint_counts.items():\n",
    "    print(f\"{timepoint}: {count:,} ({count/len(merged_snap):.1%})\")\n",
    "\n",
    "# Cases with both baseline and outcomes (with valid data)\n",
    "baseline_cases = set(merged_snap[(merged_snap['snap_Survey completed:'] == 'at the start of support') & valid_snap]['case_ref'])\n",
    "outcome_cases = set(merged_snap[(merged_snap['snap_Survey completed:'] == 'at the end of support') & valid_snap]['case_ref'])\n",
    "both_timepoints = baseline_cases & outcome_cases\n",
    "\n",
    "# Also show the impact of invalid data\n",
    "all_baseline_cases = set(merged_snap[merged_snap['snap_Survey completed:'] == 'at the start of support']['case_ref'])\n",
    "all_outcome_cases = set(merged_snap[merged_snap['snap_Survey completed:'] == 'at the end of support']['case_ref'])\n",
    "all_both_timepoints = all_baseline_cases & all_outcome_cases\n",
    "\n",
    "print(f\"\\n=== BASELINE + OUTCOMES AVAILABILITY ===\")\n",
    "print(f\"Total unique cases with valid data: {len(baseline_cases | outcome_cases):,}\")\n",
    "\n",
    "print(f\"\\nCases with valid baseline only: {len(baseline_cases - outcome_cases):,}\")\n",
    "print(f\"Cases with valid outcomes only: {len(outcome_cases - baseline_cases):,}\")\n",
    "\n",
    "print(f\"\\nCases with both valid baseline + outcomes: {len(both_timepoints):,}\")\n",
    "print(f\"Cases with both timepoints (any validity): {len(all_both_timepoints):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd85520",
   "metadata": {},
   "source": [
    "Many cases also have more than two observations. We can investigate these patterns. Also, from here onwards, I filter the SNAP dataset to valid SNAP records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2357d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE + OUTCOMES CASES ANALYSIS (n=472) ===\n",
      "\n",
      "Referral notes length distribution:\n",
      "  Long note (20+ words): 167 (35.4%)\n",
      "  Short note (<5 words): 120 (25.4%)\n",
      "  Medium note (5-19 words): 119 (25.2%)\n",
      "  No note: 66 (14.0%)\n"
     ]
    }
   ],
   "source": [
    "merged_snap_valid = merged_snap[merged_snap['snap_Possible to record outcomes:'] == 'Yes'].copy()\n",
    "\n",
    "# Focus on the 472 cases with both baseline + outcomes\n",
    "both_timepoints_snap = merged_snap_valid[merged_snap_valid['case_ref'].isin(both_timepoints)].drop_duplicates('case_ref')\n",
    "\n",
    "print(f\"=== BASELINE + OUTCOMES CASES ANALYSIS (n={len(both_timepoints):,}) ===\")\n",
    "\n",
    "length_distribution = both_timepoints_snap['note_length_category'].value_counts()\n",
    "\n",
    "print(f\"\\nReferral notes length distribution:\")\n",
    "for category, count in length_distribution.items():\n",
    "    print(f\"  {category}: {count:,} ({count/len(both_timepoints_snap):.1%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55cdfc",
   "metadata": {},
   "source": [
    "Let's now check the completeness of the data for cases which have both baseline and post-support observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bc7093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA COMPLETENESS ===\n",
      "Control: 90.1% complete\n",
      "Personal cleanliness: 91.3% complete\n",
      "Food and drink: 91.5% complete\n",
      "Personal safety: 92.3% complete\n",
      "Social Participation: 90.2% complete\n",
      "Occupation: 89.7% complete\n",
      "Accommodation: 91.0% complete\n",
      "Dignity 1: 89.2% complete\n",
      "Medication: 89.6% complete\n",
      "Finances: 86.8% complete\n"
     ]
    }
   ],
   "source": [
    "# Data completeness for key domains\n",
    "key_domains = ['snap_Control (QLC)', 'snap_Personal cleanliness (QLC)', 'snap_Food and drink (QLC)', \n",
    "               'snap_Personal safety (QLC)', 'snap_Social Participation (QLC)', 'snap_Occupation (QLC)', \n",
    "               'snap_Accommodation (QLC)', 'snap_Dignity 1 (QLC)', 'snap_Medication (QLC)', 'snap_Finances (QLC)']\n",
    "\n",
    "print(f\"\\n=== DATA COMPLETENESS ===\")\n",
    "for domain in key_domains:\n",
    "    if domain in merged_snap_valid.columns:\n",
    "        completeness = merged_snap_valid[domain].notna().mean()\n",
    "        print(f\"{domain.replace('snap_', '').replace(' (QLC)', '')}: {completeness:.1%} complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9b12f",
   "metadata": {},
   "source": [
    "Some cases appear more than twice (baseline + post-support). They also seem to display different change values across their associated observations, i.e., these are not simply duplicated rows. \n",
    "\n",
    "However, there is only one referral note associated with each CAS. So, this discrepancy will have to be dealt with. Probably by using the values appear first in chronological order. This is because the BRC will have more information on the service user than contained in the referral note if they use their services for the second or third time for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab904b",
   "metadata": {},
   "source": [
    "### 3.2. HIU (High Intensity Use service for people frequently attending A&E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bcb0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset to only include referrals with HIU data\n",
    "merged_hiu = merged[merged['has_hiu']].copy()\n",
    "merged_hiu = merged_hiu.loc[:, ~merged_hiu.columns.str.startswith('snap_')]\n",
    "\n",
    "cols_hiu = list(merged_hiu.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2de38be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HIU DATA OVERVIEW ===\n",
      "Total HIU observations: 4,269\n",
      "Unique cases: 1,400\n",
      "\n",
      "=== DUPLICATE ANALYSIS ===\n",
      "Total rows: 4,269\n",
      "Exact duplicate rows (case + HIU variables): 2,932\n",
      "\n",
      "Example of duplicate cases:\n",
      "  CAS-525447: 27 identical rows\n",
      "  CAS-527026: 24 identical rows\n",
      "  CAS-482848: 20 identical rows\n",
      "\n",
      "=== REMOVING DUPLICATES ===\n",
      "After removing duplicates: 2,370 rows\n",
      "Duplicates removed: 1,899\n",
      "Unique cases after cleaning: 1,400\n"
     ]
    }
   ],
   "source": [
    "print(\"=== HIU DATA OVERVIEW ===\")\n",
    "print(f\"Total HIU observations: {len(merged_hiu):,}\")\n",
    "print(f\"Unique cases: {merged_hiu['case_ref'].nunique():,}\")\n",
    "\n",
    "# Check for exact duplicates\n",
    "print(f\"\\n=== DUPLICATE ANALYSIS ===\")\n",
    "print(f\"Total rows: {len(merged_hiu):,}\")\n",
    "\n",
    "# Check duplicates based on case reference and all HIU variables\n",
    "hiu_cols = [col for col in merged_hiu.columns if col.startswith('hiu_')]\n",
    "duplicate_subset = ['case_ref'] + hiu_cols\n",
    "\n",
    "duplicates = merged_hiu.duplicated(subset=duplicate_subset, keep=False)\n",
    "print(f\"Exact duplicate rows (case + HIU variables): {duplicates.sum():,}\")\n",
    "\n",
    "if duplicates.sum() > 0:\n",
    "    print(f\"\\nExample of duplicate cases:\")\n",
    "    duplicate_cases = merged_hiu[duplicates]['case_ref'].value_counts().head(3)\n",
    "    for case_ref, count in duplicate_cases.items():\n",
    "        print(f\"  {case_ref}: {count} identical rows\")\n",
    "\n",
    "# Remove exact duplicates\n",
    "print(f\"\\n=== REMOVING DUPLICATES ===\")\n",
    "merged_hiu_clean = merged_hiu.drop_duplicates(subset=duplicate_subset, keep='first')\n",
    "print(f\"After removing duplicates: {len(merged_hiu_clean):,} rows\")\n",
    "print(f\"Duplicates removed: {len(merged_hiu) - len(merged_hiu_clean):,}\")\n",
    "print(f\"Unique cases after cleaning: {merged_hiu_clean['case_ref'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "337361b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid HIU data: 2,231 observations (94.1%)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'case_ref'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:175\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'case_ref'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m merged_hiu_valid \u001b[38;5;241m=\u001b[39m merged_hiu_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhiu_Q6. Why wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt it possible to record outcomes for this client?\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValid HIU data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmerged_hiu_valid\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m observations (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmerged_hiu_valid\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid unique cases: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmerged_hiu_valid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcase_ref\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'case_ref'"
     ]
    }
   ],
   "source": [
    "# Check for valid HIU data\n",
    "# Looking at the columns, it seems like \"hiu_Q6. Why wasn't it possible to record outcomes for this client?\" indicates invalid data\n",
    "merged_hiu_valid = merged_hiu_clean[\"hiu_Q6. Why wasn't it possible to record outcomes for this client?\"].isna()\n",
    "print(f\"\\nValid HIU data: {merged_hiu_valid.sum():,} observations ({merged_hiu_valid.mean():.1%})\")\n",
    "print(f\"Valid unique cases: {merged_hiu_valid['case_ref'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a880eb",
   "metadata": {},
   "source": [
    "## 3. Cleaning referrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cleaning_helpers import clean_na_variations, remove_duplicate_sentences_per_case\n",
    "\n",
    "brc_referrals_raw = pd.read_csv(\"../data/raw/BRC-Data/Cases_depersonalised.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149581f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (181085, 19)\n",
      "Columns: ['Area', 'Scheme', 'Case Reference', 'Assessment Result', 'Case Status', 'Referral Date/Time', 'End Date Case', 'Has Disability', 'Has Risk', 'Risk Type', 'Unique Case', 'IMD_decile', 'Country', 'Age', 'Gender', 'Ethnicity', 'Disability', 'Living Arrangements', 'Referral Notes (depersonalised)']\n",
      "\n",
      "Original referral notes missing values: 30621\n",
      "After removing missing referral notes: (150464, 19)\n",
      "After removing NA variations: (150464, 19) (removed 0 rows) \n",
      "\n",
      "Removing duplicate sentences within cases...\n",
      "Original sentences: 340393\n",
      "Unique sentences: 182168\n",
      "Sentences removed: 158225\n",
      "\n",
      "Final cleaned data shape: (99560, 19)\n",
      "Number of unique referral notes: 51658\n",
      "\n",
      "Cleaned data saved to: ../data/processed/BRC_referrals_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data shape:\", brc_referrals_raw.shape)\n",
    "print(\"Columns:\", brc_referrals_raw.columns.tolist())\n",
    "\n",
    "# 1. Remove missing values for referral notes\n",
    "print(f\"\\nOriginal referral notes missing values: {brc_referrals_raw['Referral Notes (depersonalised)'].isnull().sum()}\")\n",
    "\n",
    "brc_cleaned = brc_referrals_raw.dropna(subset=['Referral Notes (depersonalised)']).copy()\n",
    "print(f\"After removing missing referral notes: {brc_cleaned.shape}\")\n",
    "\n",
    "# 2. Apply NA cleaning to referral notes\n",
    "brc_cleaned['Referral Notes (depersonalised)'] = brc_cleaned['Referral Notes (depersonalised)'].apply(clean_na_variations)\n",
    "    \n",
    "# Remove rows where referral notes became NaN after cleaning\n",
    "before_na_clean = brc_cleaned.shape[0]\n",
    "brc_cleaned = brc_cleaned.dropna(subset=['Referral Notes (depersonalised)'])\n",
    "print(f\"After removing NA variations: {brc_cleaned.shape} (removed {before_na_clean - brc_cleaned.shape[0]} rows) \\n\")\n",
    "\n",
    "# 3. Remove duplicate sentences within multiple notes corresponding to a single Case Reference\n",
    "# As done in Keloth et al. (2025)\n",
    "brc_final = remove_duplicate_sentences_per_case(brc_cleaned)\n",
    "print(f\"\\nFinal cleaned data shape: {brc_final.shape}\")\n",
    "print(f\"Number of unique referral notes: {brc_final['Referral Notes (depersonalised)'].nunique()}\")\n",
    "\n",
    "# Save the cleaned data\n",
    "output_path = Path(\"../data/processed/BRC_referrals_cleaned.csv\")\n",
    "brc_final.to_csv(output_path, index=False)\n",
    "print(f\"\\nCleaned data saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
