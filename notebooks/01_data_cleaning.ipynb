{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e413d4d",
   "metadata": {},
   "source": [
    "# Data cleaning & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cf3e",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9bee6b",
   "metadata": {},
   "source": [
    "## 1. Duplicate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and standardize case references\n",
    "datasets = {\n",
    "    'referrals': pd.read_csv(\"../data/raw/BRC-Data/Cases_depersonalised.csv\"),\n",
    "    'hiu': pd.read_csv(\"../data/raw/BRC-Data/HIU_depersonalised.csv\"),\n",
    "    'snap': pd.read_csv(\"../data/raw/BRC-Data/SNAP_depersonalised.csv\")\n",
    "}\n",
    "\n",
    "# Standardize case reference columns\n",
    "datasets['referrals']['case_ref'] = datasets['referrals']['Case Reference']\n",
    "datasets['hiu']['case_ref'] = 'CAS-' + datasets['hiu']['Q2.1. CAS-'].astype(str).str.replace('.0', '', regex=False)\n",
    "datasets['snap']['case_ref'] = datasets['snap']['BRM case number:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DUPLICATE ANALYSIS BEFORE CLEANING ===\")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()} Dataset:\")\n",
    "    print(f\"  Total rows: {len(df):,}\")\n",
    "    print(f\"  Unique case_ref: {df['case_ref'].nunique():,}\")\n",
    "    \n",
    "    # Define columns to exclude from duplication analysis (depersonalized/randomized)\n",
    "    if name == 'referrals':\n",
    "        # For referrals: only consider case_ref and referral notes\n",
    "        analysis_cols = ['case_ref', 'Referral Notes (depersonalised)', 'Referral Date/Time']\n",
    "        available_cols = [col for col in analysis_cols if col in df.columns]\n",
    "        print(f\"  Analysing duplicates based on: {available_cols}\")\n",
    "    elif name == 'snap':\n",
    "        # Exclude: Has Disability, IMD Decile, Country, Age, Gender, Ethnicity, Living Arrangements\n",
    "        exclude_cols = ['Has Disability', 'IMD Decile', 'Country', 'Age', 'Gender', 'Ethnicity', 'Living Arrangements']\n",
    "        available_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        print(f\"  Analysing duplicates excluding {len(exclude_cols)} depersonalised columns\")\n",
    "    elif name == 'hiu':\n",
    "        # Exclude: Age, Gender, Ethnicity, Living Arrangements\n",
    "        exclude_cols = ['Age', 'Gender', 'Ethnicity', 'Living Arrangements']\n",
    "        available_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "        print(f\"  Analysing duplicates excluding {len(exclude_cols)} depersonalised columns\")\n",
    "    \n",
    "    # Check for perfect duplicates based on relevant columns only\n",
    "    perfect_duplicates = df.duplicated(subset=available_cols).sum()\n",
    "    print(f\"  Perfect duplicates (relevant columns): {perfect_duplicates:,}\")\n",
    "    \n",
    "    # Check for duplicates by case_ref only\n",
    "    case_ref_duplicates = df['case_ref'].duplicated().sum()\n",
    "    print(f\"  Duplicate case_ref: {case_ref_duplicates:,}\")\n",
    "    \n",
    "    if case_ref_duplicates > 0:\n",
    "        # Show examples of duplicate case_ref: Show top 5 by count\n",
    "        duplicate_cases = df[df['case_ref'].duplicated(keep=False)]['case_ref'].value_counts().head(5)\n",
    "        print(f\"  Top 5 duplicate case_ref by count:\")\n",
    "        for case_ref, count in duplicate_cases.items():\n",
    "            print(f\"    {case_ref}: {count} rows\")\n",
    "        \n",
    "        # Check if duplicate case_ref have identical relevant data - CHANGED: Use highest count case\n",
    "        highest_count_case = duplicate_cases.index[0]  # This is now the case with most duplicates\n",
    "        duplicate_rows = df[df['case_ref'] == highest_count_case]\n",
    "        \n",
    "        # Check duplicates based on relevant columns only\n",
    "        duplicate_subset = duplicate_rows[available_cols]\n",
    "        identical_duplicates = duplicate_subset.duplicated().sum()\n",
    "        \n",
    "        print(f\"    For case {highest_count_case}: {identical_duplicates}/{len(duplicate_rows)-1} duplicates are identical (relevant data)\")\n",
    "        \n",
    "        # Show what's different in duplicate rows (if any) - only relevant columns\n",
    "        if identical_duplicates < len(duplicate_rows) - 1:\n",
    "            print(f\"    Non-identical relevant columns for {highest_count_case}:\")\n",
    "            for col in available_cols:\n",
    "                if col in duplicate_rows.columns:\n",
    "                    unique_vals = duplicate_rows[col].nunique()\n",
    "                    if unique_vals > 1:\n",
    "                        values = duplicate_rows[col].tolist()\n",
    "                        # For referral notes, show preview\n",
    "                        if col == 'Referral Notes (depersonalised)':\n",
    "                            values = [str(v)[:50] + \"...\" if len(str(v)) > 50 else str(v) for v in values]\n",
    "                        print(f\"      {col}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbeb98f",
   "metadata": {},
   "source": [
    "## 2. Pre-cleaning referrals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e46c7b",
   "metadata": {},
   "source": [
    "I start with the pre-cleaning of referrals.\n",
    "\n",
    "1. I remove identical rows based on: ['case_ref', 'Referral Notes (depersonalised)', 'Referral Date/Time']. Those are pure duplicates (other columns might be different due to the depersonalisation).\n",
    "\n",
    "Then, for each CAS which has multiple rows (most likely on different dates):\n",
    "\n",
    "2. I create two columns: one with the number of observations, and one with the date range of those observations;\n",
    "3. Then, if one or some of the rows have a non-NA referral, I remove the rows which don't. \n",
    "4. Finally, if all observations have the same referrals, or if none of the observations have a referral, I only keep the most recent row (the columns created earlier will keep the other relevant information).\n",
    "\n",
    "The only duplicated CAS left will be those with different referrals on different dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_cleaning.data_cleaning_helpers import clean_referrals_dataset\n",
    "\n",
    "# Load the referrals data\n",
    "referrals_df = datasets['referrals'].copy()\n",
    "\n",
    "# Clean the dataset\n",
    "referrals_cleaned = clean_referrals_dataset(referrals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ANALYSING REFERRALS BY DATE PATTERNS ===\")\n",
    "\n",
    "# Convert date column to datetime if not already\n",
    "referrals_cleaned['Referral Date/Time'] = pd.to_datetime(referrals_cleaned['Referral Date/Time'], errors='coerce')\n",
    "referrals_cleaned['referral_date'] = referrals_cleaned['Referral Date/Time'].dt.date\n",
    "\n",
    "# Cases with multiple referrals\n",
    "multi_referral_cases = referrals_cleaned['case_ref'].value_counts()\n",
    "multi_referral_cases = multi_referral_cases[multi_referral_cases > 1]\n",
    "\n",
    "print(f\"Cases with multiple referrals: {len(multi_referral_cases):,}\")\n",
    "print(f\"Cases with single referral: {referrals_cleaned['case_ref'].nunique() - len(multi_referral_cases):,}\")\n",
    "\n",
    "# Analyze date patterns for multi-referral cases\n",
    "print(f\"\\n=== DATE PATTERNS FOR {len(multi_referral_cases):,} MULTI-REFERRAL CASES ===\")\n",
    "\n",
    "same_date_stats = []\n",
    "for case_ref in multi_referral_cases.index:\n",
    "    case_data = referrals_cleaned[referrals_cleaned['case_ref'] == case_ref].copy()\n",
    "    \n",
    "    # Count unique dates for this case\n",
    "    unique_dates = case_data['referral_date'].nunique()\n",
    "    total_referrals = len(case_data)\n",
    "    \n",
    "    same_date_stats.append({\n",
    "        'case_ref': case_ref,\n",
    "        'total_referrals': total_referrals,\n",
    "        'unique_dates': unique_dates,\n",
    "        'all_same_date': unique_dates == 1,\n",
    "        'multiple_dates': unique_dates > 1\n",
    "    })\n",
    "\n",
    "same_date_df = pd.DataFrame(same_date_stats)\n",
    "\n",
    "# Summary statistics\n",
    "all_same_date = same_date_df['all_same_date'].sum()\n",
    "multiple_dates = same_date_df['multiple_dates'].sum()\n",
    "\n",
    "print(f\"Cases where ALL referrals are on the SAME date: {all_same_date:,} ({all_same_date/len(multi_referral_cases)*100:.1f}%)\")\n",
    "print(f\"Cases with referrals on MULTIPLE dates: {multiple_dates:,} ({multiple_dates/len(multi_referral_cases)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7dde9a",
   "metadata": {},
   "source": [
    "For cases which have multiple observations with DIFFERENT referrals, they are always done on the same date. I decide to keep the longest referral, to keep the maximum information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e082643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cleaning_helpers import consolidate_referrals_longest\n",
    "\n",
    "# Apply consolidation\n",
    "referrals_consolidated = consolidate_referrals_longest(referrals_cleaned)\n",
    "\n",
    "# Save consolidated dataset\n",
    "referrals_consolidated.to_csv(\"../data/processed/referrals_cleaned.csv\", index=False)\n",
    "print(f\"\\nConsolidated referrals saved to: ../data/processed/referrals_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81666df",
   "metadata": {},
   "source": [
    "## 3. Pre-cleaning SNAP data & merging with referrals\n",
    "\n",
    "I start with **SNAP** (Support at Home, Care at Home, Hospital at Home, and Social Prescribing services).\n",
    "\n",
    "These services have started using an outcomes framework called hiu (Social Needs and Preferences), which was only introduced in late 2024.\n",
    "\n",
    "The pre-cleaning of SNAP is done as follows:\n",
    "\n",
    "1. I removed perfect duplicates based on columns that were not randomised in the depersonalisation process.\n",
    "\n",
    "Then, I noticed that CAS have either one or two observations (not more). These correspond to observations at the start and / or at the end of support from the BRC. I get a brief overview.\n",
    "\n",
    "2. For each case, I determine what type of valid assessments are available, only counting valid where 'Possible to record outcomes:' == 'Yes'. \n",
    "3. I then create a summary for each case showing:\n",
    "    - Total number of assessments\n",
    "    - Whether valid baseline assessment exists (timepoint 1.0 + recordable outcomes)\n",
    "    - Whether valid post-support assessment exists (timepoint 2.0 + recordable outcomes)\n",
    "    - Whether case has both valid assessments (complete usable pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b26266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cleaning_helpers import clean_snap_dataset\n",
    "\n",
    "snap_df = datasets['snap'].copy()\n",
    "\n",
    "# Clean the SNAP dataset\n",
    "snap_cleaned = clean_snap_dataset(snap_df)\n",
    "\n",
    "# Save cleaned dataset\n",
    "snap_cleaned.to_csv(\"../data/processed/snap_cleaned.csv\", index=False)\n",
    "print(f\"\\nCleaned dataset saved to: ../data/processed/snap_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbec9b1",
   "metadata": {},
   "source": [
    "### Merging with SNAP & referrals\n",
    "\n",
    "I now proceed to creating a merged dataset with referrals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f50245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_referrals_with_snap():\n",
    "    \"\"\"\n",
    "    Merge referrals (1 row per case) with SNAP data (1-2 rows per case).\n",
    "    Only keeps cases that appear in both datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== MERGING REFERRALS WITH SNAP DATA ===\")\n",
    "    \n",
    "    # Load datasets\n",
    "    referrals_df = pd.read_csv(\"../data/processed/referrals_cleaned.csv\")\n",
    "    snap_df = pd.read_csv(\"../data/processed/snap_cleaned.csv\")\n",
    "    \n",
    "    print(f\"Referrals dataset: {len(referrals_df):,} rows, {referrals_df['case_ref'].nunique():,} unique cases\")\n",
    "    print(f\"SNAP dataset: {len(snap_df):,} rows, {snap_df['case_ref'].nunique():,} unique cases\")\n",
    "    \n",
    "    # Find cases that appear in both datasets\n",
    "    referrals_cases = set(referrals_df['case_ref'])\n",
    "    snap_cases = set(snap_df['case_ref'])\n",
    "    \n",
    "    common_cases = referrals_cases.intersection(snap_cases)\n",
    "    \n",
    "    print(f\"\\nCases in both datasets: {len(common_cases):,}\")\n",
    "    print(f\"Cases only in referrals: {len(referrals_cases - snap_cases):,}\")\n",
    "    print(f\"Cases only in SNAP: {len(snap_cases - referrals_cases):,}\")\n",
    "    \n",
    "    # Filter both datasets to common cases only\n",
    "    referrals_common = referrals_df[referrals_df['case_ref'].isin(common_cases)].copy()\n",
    "    snap_common = snap_df[snap_df['case_ref'].isin(common_cases)].copy()\n",
    "    \n",
    "    print(f\"\\nAfter filtering to common cases:\")\n",
    "    print(f\"Referrals: {len(referrals_common):,} rows\")\n",
    "    print(f\"SNAP: {len(snap_common):,} rows\")\n",
    "    \n",
    "    # Merge: each referral row will be duplicated for each SNAP row of the same case\n",
    "    merged_df = snap_common.merge(referrals_common, on='case_ref', how='inner', suffixes=('_snap', '_referral'))\n",
    "    \n",
    "    print(f\"\\nMerged dataset: {len(merged_df):,} rows, {merged_df['case_ref'].nunique():,} unique cases\")\n",
    "    \n",
    "    # Show structure breakdown\n",
    "    rows_per_case = merged_df['case_ref'].value_counts()\n",
    "    print(f\"\\nRows per case in merged dataset:\")\n",
    "    print(f\"  1 row (baseline only): {(rows_per_case == 1).sum():,} cases\")\n",
    "    print(f\"  2 rows (baseline + outcome): {(rows_per_case == 2).sum():,} cases\")\n",
    "    if (rows_per_case > 2).any():\n",
    "        print(f\"  >2 rows (unexpected): {(rows_per_case > 2).sum():,} cases\")\n",
    "    \n",
    "    # Show SNAP assessment validity breakdown for merged cases\n",
    "    if 'has_both' in merged_df.columns:\n",
    "        cases_with_both = merged_df.drop_duplicates('case_ref')['has_both'].sum()\n",
    "        cases_baseline_only = merged_df.drop_duplicates('case_ref')['has_valid_baseline'].sum() - cases_with_both\n",
    "        \n",
    "        print(f\"\\nSNAP assessment validity in merged dataset:\")\n",
    "        print(f\"  Cases with valid baseline + outcome: {cases_with_both:,}\")\n",
    "        print(f\"  Cases with valid baseline only: {cases_baseline_only:,}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Perform the merge\n",
    "merged_referrals_snap = merge_referrals_with_snap()\n",
    "\n",
    "# Save merged dataset\n",
    "merged_referrals_snap.to_csv(\"../data/processed/merged_referrals_snap.csv\", index=False)\n",
    "print(f\"\\nMerged dataset saved to: ../data/processed/merged_referrals_snap.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55cdfc",
   "metadata": {},
   "source": [
    "Let's now check the completeness of the data for cases which have both baseline and post-support observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data completeness for key domains\n",
    "key_domains = ['Control (QLC)', 'Personal cleanliness (QLC)', 'Food and drink (QLC)', \n",
    "               'Personal safety (QLC)', 'Social Participation (QLC)', 'Occupation (QLC)', \n",
    "               'Accommodation (QLC)', 'Dignity 1 (QLC)', 'snap_Medication (QLC)', 'Finances (QLC)']\n",
    "\n",
    "# Filter rows with both baseline and outcome\n",
    "filtered_snap = merged_referrals_snap[merged_referrals_snap['has_both']]\n",
    "\n",
    "print(f\"\\n=== DATA COMPLETENESS ===\")\n",
    "for domain in key_domains:\n",
    "    if domain in filtered_snap.columns:\n",
    "        completeness = filtered_snap[domain].notna().mean()\n",
    "        print(f\"{domain}: {completeness:.1%} complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc008f",
   "metadata": {},
   "source": [
    "## 4. Pre-cleaning HIU data & merging with referrals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab904b",
   "metadata": {},
   "source": [
    "I move on to **HIU** (High Intensity Use service for people frequently attending A&E).\n",
    "\n",
    "1. Same as before, I first remove perfect duplicates, excluding the 4 depersonalized columns (Age, Gender, Ethnicity, Living Arrangements) from duplicate detection since these were randomised.\n",
    "2. I then identify valid assessments by 'Time Points'. This step uses the 'Q6. Why wasn't it possible to record outcomes for this client?' column - if this is filled out, outcomes were NOT recordable, so we need this to be empty/NaN for valid assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cleaning_helpers import clean_hiu_dataset\n",
    "\n",
    "hiu_df = datasets['hiu'].copy()\n",
    "\n",
    "hiu_cleaned = clean_hiu_dataset(hiu_df)\n",
    "\n",
    "# Save cleaned dataset\n",
    "hiu_cleaned.to_csv(\"../data/processed/hiu_cleaned.csv\", index=False)\n",
    "print(f\"\\nCleaned dataset saved to: ../data/processed/hiu_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data completeness for key domains\n",
    "key_domains2 = ['Change in Activation', 'Change in Wellbeing', 'Change in Housing', \n",
    "               'Change in Finance', 'Change in Loneliness', 'Change in Social Value']\n",
    "\n",
    "# Filter rows with both baseline and outcome\n",
    "filtered_hiu = hiu_cleaned[hiu_cleaned['has_both']]\n",
    "\n",
    "print(f\"\\n=== DATA COMPLETENESS ===\")\n",
    "for domain in key_domains2:\n",
    "    if domain in filtered_hiu.columns:\n",
    "        completeness = filtered_hiu[domain].notna().mean()\n",
    "        print(f\"{domain}: {completeness:.1%} complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3081400d",
   "metadata": {},
   "source": [
    "### Merging HIU & referrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bc1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_referrals_with_hiu():\n",
    "    \"\"\"\n",
    "    Merge referrals (1 row per case) with HIU data (multiple rows per case).\n",
    "    Only keeps cases that appear in both datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== MERGING REFERRALS WITH HIU DATA ===\")\n",
    "    \n",
    "    # Load datasets\n",
    "    referrals_df = pd.read_csv(\"../data/processed/referrals_cleaned.csv\")\n",
    "    hiu_df = pd.read_csv(\"../data/processed/hiu_cleaned.csv\")\n",
    "    \n",
    "    print(f\"Referrals dataset: {len(referrals_df):,} rows, {referrals_df['case_ref'].nunique():,} unique cases\")\n",
    "    print(f\"HIU dataset: {len(hiu_df):,} rows, {hiu_df['case_ref'].nunique():,} unique cases\")\n",
    "    \n",
    "    # Find cases that appear in both datasets\n",
    "    referrals_cases = set(referrals_df['case_ref'])\n",
    "    hiu_cases = set(hiu_df['case_ref'])\n",
    "    \n",
    "    common_cases = referrals_cases.intersection(hiu_cases)\n",
    "    \n",
    "    print(f\"\\nCases in both datasets: {len(common_cases):,}\")\n",
    "    print(f\"Cases only in referrals: {len(referrals_cases - hiu_cases):,}\")\n",
    "    print(f\"Cases only in HIU: {len(hiu_cases - referrals_cases):,}\")\n",
    "    \n",
    "    # Filter both datasets to common cases only\n",
    "    referrals_common = referrals_df[referrals_df['case_ref'].isin(common_cases)].copy()\n",
    "    hiu_common = hiu_df[hiu_df['case_ref'].isin(common_cases)].copy()\n",
    "    \n",
    "    print(f\"\\nAfter filtering to common cases:\")\n",
    "    print(f\"Referrals: {len(referrals_common):,} rows\")\n",
    "    print(f\"HIU: {len(hiu_common):,} rows\")\n",
    "    \n",
    "    # Merge: each referral row will be duplicated for each HIU row of the same case\n",
    "    merged_df = hiu_common.merge(referrals_common, on='case_ref', how='inner', suffixes=('_hiu', '_referral'))\n",
    "    \n",
    "    print(f\"\\nMerged dataset: {len(merged_df):,} rows, {merged_df['case_ref'].nunique():,} unique cases\")\n",
    "    \n",
    "    # Show structure breakdown\n",
    "    rows_per_case = merged_df['case_ref'].value_counts()\n",
    "    print(f\"\\nRows per case in merged dataset:\")\n",
    "    print(f\"  1 row (baseline only): {(rows_per_case == 1).sum():,} cases\")\n",
    "    print(f\"  2 rows (baseline + outcome): {(rows_per_case == 2).sum():,} cases\")\n",
    "    if (rows_per_case > 2).any():\n",
    "        print(f\"  >2 rows (unexpected): {(rows_per_case > 2).sum():,} cases\")\n",
    "    \n",
    "    # Show HIU assessment validity breakdown for merged cases\n",
    "    if 'has_both' in merged_df.columns:\n",
    "        cases_with_both = merged_df.drop_duplicates('case_ref')['has_both'].sum()\n",
    "        cases_baseline_only = merged_df.drop_duplicates('case_ref')['has_valid_baseline'].sum() - cases_with_both\n",
    "        \n",
    "        print(f\"\\HIU assessment validity in merged dataset:\")\n",
    "        print(f\"  Cases with valid baseline + outcome: {cases_with_both:,}\")\n",
    "        print(f\"  Cases with valid baseline only: {cases_baseline_only:,}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Perform the merge\n",
    "merged_referrals_hiu = merge_referrals_with_hiu()\n",
    "\n",
    "# Save merged dataset\n",
    "merged_referrals_hiu.to_csv(\"../data/processed/merged_referrals_hiu.csv\", index=False)\n",
    "print(f\"\\nMerged dataset saved to: ../data/processed/merged_referrals_hiu.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
