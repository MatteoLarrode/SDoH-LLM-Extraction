{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d71a71a",
   "metadata": {},
   "source": [
    "# SDoH Extraction from MIMIC-III Annotated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb2a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b3ed5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cached models:\n",
      "  CohereForAI/aya-23-35B\n",
      "  CohereForAI/aya-23-8B\n",
      "  CohereForAI/aya-vision-8b\n",
      "  HuggingFaceTB/SmolLM-135M-Instruct\n",
      "  LLaMAX/LLaMAX3-8B-Alpaca\n",
      "  Qwen/Qwen2.5-1.5B\n",
      "  Qwen/Qwen2.5-3B\n",
      "  Qwen/Qwen2.5-72B-Instruct\n",
      "  Qwen/Qwen2.5-7B\n",
      "  Qwen/Qwen2.5-7B-Instruct\n",
      "  Qwen/Qwen2.5-7B-instruct\n",
      "  Qwen/Qwen2.5-VL-7B-Instruct\n",
      "  Unbabel/wmt20-comet-qe-da\n",
      "  Unbabel/wmt22-comet-da\n",
      "  bert-base-uncased\n",
      "  bert-large-uncased\n",
      "  cardiffnlp/twitter-roberta-base-sentiment\n",
      "  clairebarale/refugee_cases_ner\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Llama-70B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "  facebook/nllb-200-3.3B\n",
      "  facebook/nllb-200-distilled-1.3B\n",
      "  facebook/nllb-200-distilled-600M\n",
      "  gpt2\n",
      "  gpt2-medium\n",
      "  gpt2-xl\n",
      "  hfl/chinese-electra-180g-small-discriminator\n",
      "  hfl/chinese-legal-electra-base-discriminator\n",
      "  hfl/chinese-legal-electra-small-discriminator\n",
      "  hfl/chinese-roberta-wwm-ext\n",
      "  hfl/chinese-roberta-wwm-ext-large\n",
      "  jxm/gtr__nq__32\n",
      "  jxm/gtr__nq__32__correct\n",
      "  meta-llama/Llama-2-7b-chat-hf\n",
      "  meta-llama/Llama-2-7b-hf\n",
      "  meta-llama/Llama-3.1-70B-Instruct\n",
      "  meta-llama/Llama-3.1-8B\n",
      "  meta-llama/Llama-3.1-8B-Instruct\n",
      "  meta-llama/Llama-3.2-11B-Vision-Instruct\n",
      "  meta-llama/Llama-3.3-70B-Instruct\n",
      "  meta-llama/Llama-4-Scout-17B-16E\n",
      "  meta-llama/Meta-Llama-3-70B-Instruct\n",
      "  meta-llama/Meta-Llama-3-8B\n",
      "  meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "  microsoft/Phi-3-mini-4k-instruct\n",
      "  microsoft/Phi-3.5-vision-instruct\n",
      "  microsoft/Phi-4-mini-instruct\n",
      "  microsoft/Phi-4-multimodal-instruct\n",
      "  microsoft/deberta-v3-base\n",
      "  microsoft/deberta-xlarge\n",
      "  microsoft/deberta-xlarge-mnli\n",
      "  mistral-community/pixtral-12b\n",
      "  mistralai/Mistral-7B-Instruct-v0.2\n",
      "  mistralai/Mistral-7B-Instruct-v0.3\n",
      "  mistralai/Mistral-7B-v0.1\n",
      "  mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "  mosaicml/mpt-7b-chat\n",
      "  nlpaueb/legal-bert-base-uncased\n",
      "  nvidia/Llama-3.1-Nemotron-Nano-8B-v1\n",
      "  openai-community/gpt2\n",
      "  openbmb/MiniCPM-o-2_6\n",
      "  roberta-base\n",
      "  saibo/legal-roberta-base\n",
      "  sentence-transformers/LaBSE\n",
      "  sentence-transformers/all-MiniLM-L6-v2\n",
      "  sentence-transformers/all-mpnet-base-v2\n",
      "  sentence-transformers/gtr-t5-base\n",
      "  sentence-transformers/paraphrase-distilroberta-base-v2\n",
      "  sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "  t5-base\n",
      "  t5-large\n",
      "  t5-small\n",
      "  xlm-roberta-base\n",
      "  xlm-roberta-large\n"
     ]
    }
   ],
   "source": [
    "# Use shared cache\n",
    "os.environ['HF_HOME'] = '/data/resource/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data/resource/huggingface/hub'\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'  # Force offline mode\n",
    "\n",
    "# What models are available\n",
    "cache_dir = \"/data/resource/huggingface/hub\"\n",
    "available_models = []\n",
    "\n",
    "if os.path.exists(cache_dir):\n",
    "    for item in os.listdir(cache_dir):\n",
    "        if item.startswith(\"models--\"):\n",
    "            # Convert models--org--name to org/name format\n",
    "            model_name = item.replace(\"models--\", \"\").replace(\"--\", \"/\")\n",
    "            available_models.append(model_name)\n",
    "\n",
    "print(\"Available cached models:\")\n",
    "for model in sorted(available_models):\n",
    "    print(f\"  {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c75c022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.3\n",
      "PyTorch version: 2.6.0\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c88900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample clinical text: Person is a 45-year-old unemployed male who lives alone. \n",
      "He has a history of alcohol abuse and is currently homeless.\n",
      "Person reports feeling socially isolated and has limited social support.\n",
      "He completed high school education but has no college degree.\n"
     ]
    }
   ],
   "source": [
    "# Test text with SDoH factors\n",
    "text = \"\"\"\n",
    "Person is a 45-year-old unemployed male who lives alone. \n",
    "He has a history of alcohol abuse and is currently homeless.\n",
    "Person reports feeling socially isolated and has limited social support.\n",
    "He completed high school education but has no college degree.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nSample clinical text: {text.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb2c2ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING BERT\n",
      "==================================================\n",
      "Loading BERT tokenizer...\n",
      "✓ BERT tokenizer loaded!\n",
      "Loading BERT model...\n",
      "✓ BERT model loaded!\n",
      "Number of BERT tokens: 50\n",
      "First 10 BERT tokens: ['[CLS]', 'person', 'is', 'a', '45', '-', 'year', '-', 'old', 'unemployed']\n",
      "BERT embeddings shape: torch.Size([1, 50, 1024])\n",
      "BERT hidden size: 1024\n",
      "BERT [CLS] embedding shape: torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "# Test BERT\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING BERT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    print(\"Loading BERT tokenizer...\")\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"bert-large-uncased\", \n",
    "        local_files_only=True,\n",
    "        cache_dir=\"/data/resource/huggingface/hub\"\n",
    "    )\n",
    "    print(\"✓ BERT tokenizer loaded!\")\n",
    "    \n",
    "    print(\"Loading BERT model...\")\n",
    "    bert_model = AutoModel.from_pretrained(\n",
    "        \"bert-large-uncased\", \n",
    "        local_files_only=True,\n",
    "        cache_dir=\"/data/resource/huggingface/hub\"\n",
    "    )\n",
    "    print(\"✓ BERT model loaded!\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    bert_tokens = bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    print(f\"Number of BERT tokens: {len(bert_tokens['input_ids'][0])}\")\n",
    "    \n",
    "    # Show some tokens\n",
    "    decoded_tokens = bert_tokenizer.convert_ids_to_tokens(bert_tokens['input_ids'][0])\n",
    "    print(f\"First 10 BERT tokens: {decoded_tokens[:10]}\")\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        bert_outputs = bert_model(**bert_tokens)\n",
    "        bert_embeddings = bert_outputs.last_hidden_state\n",
    "    \n",
    "    print(f\"BERT embeddings shape: {bert_embeddings.shape}\")\n",
    "    print(f\"BERT hidden size: {bert_embeddings.shape[-1]}\")\n",
    "    \n",
    "    # Get [CLS] token embedding (sentence representation)\n",
    "    bert_cls = bert_embeddings[0, 0, :]  # [CLS] is first token\n",
    "    print(f\"BERT [CLS] embedding shape: {bert_cls.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ BERT failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f415992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING ROBERTA\n",
      "==================================================\n",
      "Loading RoBERTa tokenizer...\n",
      "✓ RoBERTa tokenizer loaded!\n",
      "Loading RoBERTa model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RoBERTa model loaded!\n",
      "Number of RoBERTa tokens: 56\n",
      "First 10 RoBERTa tokens: ['<s>', 'Ċ', 'Person', 'Ġis', 'Ġa', 'Ġ45', '-', 'year', '-', 'old']\n",
      "RoBERTa embeddings shape: torch.Size([1, 56, 768])\n",
      "RoBERTa hidden size: 768\n",
      "RoBERTa <s> embedding shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Test RoBERTa\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING ROBERTA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    print(\"Loading RoBERTa tokenizer...\")\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"roberta-base\", \n",
    "        local_files_only=True,\n",
    "        cache_dir=\"/data/resource/huggingface/hub\"\n",
    "    )\n",
    "    print(\"✓ RoBERTa tokenizer loaded!\")\n",
    "    \n",
    "    print(\"Loading RoBERTa model...\")\n",
    "    roberta_model = AutoModel.from_pretrained(\n",
    "        \"roberta-base\", \n",
    "        local_files_only=True,\n",
    "        cache_dir=\"/data/resource/huggingface/hub\"\n",
    "    )\n",
    "    print(\"✓ RoBERTa model loaded!\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    roberta_tokens = roberta_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    print(f\"Number of RoBERTa tokens: {len(roberta_tokens['input_ids'][0])}\")\n",
    "    \n",
    "    # Show some tokens\n",
    "    decoded_tokens = roberta_tokenizer.convert_ids_to_tokens(roberta_tokens['input_ids'][0])\n",
    "    print(f\"First 10 RoBERTa tokens: {decoded_tokens[:10]}\")\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        roberta_outputs = roberta_model(**roberta_tokens)\n",
    "        roberta_embeddings = roberta_outputs.last_hidden_state\n",
    "    \n",
    "    print(f\"RoBERTa embeddings shape: {roberta_embeddings.shape}\")\n",
    "    print(f\"RoBERTa hidden size: {roberta_embeddings.shape[-1]}\")\n",
    "    \n",
    "    # Get <s> token embedding (sentence representation)\n",
    "    roberta_cls = roberta_embeddings[0, 0, :]  # <s> is first token\n",
    "    print(f\"RoBERTa <s> embedding shape: {roberta_cls.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ RoBERTa failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03718836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDoH keyword detection\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SDOH KEYWORD ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sdoh_keywords = [\n",
    "    \"unemployed\", \"homeless\", \"alcohol\", \"abuse\", \"isolated\", \n",
    "    \"support\", \"alone\", \"education\", \"school\", \"college\", \"male\"\n",
    "]\n",
    "\n",
    "text_lower = text.lower()\n",
    "found_keywords = []\n",
    "for keyword in sdoh_keywords:\n",
    "    if keyword in text_lower:\n",
    "        found_keywords.append(keyword)\n",
    "        print(f\"✓ Found SDoH indicator: {keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cd036",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
