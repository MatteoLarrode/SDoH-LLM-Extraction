{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e413d4d",
   "metadata": {},
   "source": [
    "# Curation and creation of data for LLM finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cf3e",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c7d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38c32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd8299",
   "metadata": {},
   "source": [
    "## 1. LLaMA Parameter Optmisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2df90",
   "metadata": {},
   "source": [
    "*Question 1: Which model does the training save? Which performance is it based off of?*\n",
    "\n",
    "`load_best_model_at_end=True,` combined with `eval_strategy=\"epoch\", save_strategy=\"epoch\", save_total_limit=1,` means that \n",
    "\n",
    "- The model is evaluated at the end of each epoch.\n",
    "- Only the best model according to the default metric is retained at the end (save_total_limit=1 prevents clutter).\n",
    "- `Trainer` will automatically reload the best-performing checkpoint at the end based on the evaluation loss (by default).\n",
    "\n",
    "So **the model saved is the one with the lowest validation loss at the end of its epoch**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a80515",
   "metadata": {},
   "source": [
    "*Question 2: Which training/LoRA parameters can be explored to improve performance?*\n",
    "\n",
    "There are two optimisation targets:\n",
    "- LoRA configuration\n",
    "- Training hyperparameters\n",
    "\n",
    "(A) LoRA parameters (from LoraConfig) include\n",
    "- r (e.g. 4 ro 32)\n",
    "- lora_alpha (e.g. 8 to 64)\n",
    "- target modules (e.g q_proj, k_proj, v_proj, o_proj, but also gate_proj, down_proj, up_proj)\n",
    "- lora_dropout (0, or 0.05, 0.1 if overfitting)\n",
    "\n",
    "(B) Training hyperparameters (from TrainingArguments)\n",
    "- learning_rate (e.g. 9e-5 to 2e-4)\n",
    "- per_device_train_batch_size (e.g. 4 to 16)\n",
    "- num_train_epochs\n",
    "- warmup_ratio\n",
    "- lr_scheduler_type\n",
    "- weight_decay\n",
    "- gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b81c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LaTeX table with top 7 and bolded best Macro-F1 saved to ../results/latex_tables/top7_lora_search.tex\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "log_path = \"../llama_search_runs/search_progress_log.txt\"\n",
    "output_dir = Path(\"../results/latex_tables\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"top7_lora_search.tex\"\n",
    "\n",
    "# Read log\n",
    "with open(log_path, \"r\") as f:\n",
    "    log_lines = f.readlines()\n",
    "\n",
    "entries = []\n",
    "for line in log_lines:\n",
    "    run_match = re.search(r\"run_\\d+_r(\\d+)_alpha(\\d+)_drop([0-9.]+)_lr([\\deE.-]+)_bs(\\d+)\", line)\n",
    "    f1_match = re.search(r\"macro_f1: ([0-9.]+)\", line)\n",
    "    if run_match and f1_match:\n",
    "        r, alpha, drop, lr, bs = run_match.groups()\n",
    "        entries.append({\n",
    "            \"LoRA rank ($r$)\": int(r),\n",
    "            \"LoRA alpha\": int(alpha),\n",
    "            \"LoRA dropout\": float(drop),\n",
    "            \"Learning rate\": lr,\n",
    "            \"Batch size\": int(bs),\n",
    "            \"Macro-F1\": float(f1_match.group(1))\n",
    "        })\n",
    "\n",
    "# DataFrame and top 7\n",
    "df = pd.DataFrame(entries)\n",
    "top7 = df.sort_values(\"Macro-F1\", ascending=False).head(7).reset_index(drop=True)\n",
    "\n",
    "# Round values\n",
    "top7[\"LoRA dropout\"] = top7[\"LoRA dropout\"].round(2)\n",
    "top7[\"Macro-F1\"] = top7[\"Macro-F1\"].round(2)\n",
    "\n",
    "# Convert all values to string for LaTeX formatting\n",
    "top7 = top7.astype(str)\n",
    "\n",
    "# Bold only the best Macro-F1 value\n",
    "best_idx = top7[\"Macro-F1\"].astype(float).idxmax()\n",
    "top7.at[best_idx, \"Macro-F1\"] = f\"\\\\textbf{{{top7.at[best_idx, 'Macro-F1']}}}\"\n",
    "\n",
    "# Reorder columns and add vertical bar before Macro-F1\n",
    "columns = [\"LoRA rank ($r$)\", \"LoRA alpha\", \"LoRA dropout\", \"Learning rate\", \"Batch size\", \"Macro-F1\"]\n",
    "column_format = \"lllll|l\"  # vertical bar before last column\n",
    "\n",
    "# Generate LaTeX\n",
    "latex_table = top7.to_latex(index=False, escape=False, column_format=column_format)\n",
    "\n",
    "# Wrap in resizebox with caption and label\n",
    "wrapped_latex = f\"\"\"\n",
    "\\\\centering\n",
    "\\\\resizebox{{\\\\linewidth}}{{!}}{{%\n",
    "{latex_table}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Save LaTeX file\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(wrapped_latex)\n",
    "\n",
    "print(f\"✅ LaTeX table with top 7 and bolded best Macro-F1 saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc09a1",
   "metadata": {},
   "source": [
    "## 2. RoBERTa Parameter Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28788e5",
   "metadata": {},
   "source": [
    "This section describes how I fine-tuned a RoBERTa model to classify sentences as containing Social Determinants of Health (SDoH) or not.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "I use **Binary Cross-Entropy with Logits Loss** (`BCEWithLogitsLoss`) with a `pos_weight` parameter to address class imbalance:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}(z, y) = -w \\cdot \\left[y \\cdot \\log(\\sigma(z)) + (1 - y) \\cdot \\log(1 - \\sigma(z))\\right]\n",
    "\\]\n",
    "\n",
    "- \\( z \\): raw model output (logit)\n",
    "- \\( y \\in \\{0, 1\\} \\): binary label\n",
    "- \\( \\sigma(z) \\): sigmoid function\n",
    "- \\( w = \\text{pos\\_weight} \\): balancing weight, set to `#neg / #pos` in training data\n",
    "\n",
    "This setup ensures greater penalty for misclassifying positive (minority class) examples.\n",
    "\n",
    "### Tunable Parameters\n",
    "\n",
    "I explored the impact of the following hyperparameters on model performance:\n",
    "\n",
    "| Category         | Parameter                  | Description                                              | Typical Values       |\n",
    "|------------------|----------------------------|----------------------------------------------------------|----------------------|\n",
    "| **Model**        | `num_frozen_layers`        | Number of RoBERTa encoder layers to freeze               | `0`, `6`, `10`       |\n",
    "| **Training**     | `learning_rate`            | Optimizer learning rate                                  | `1e-5` to `5e-5`     |\n",
    "|                  | `num_of_epochs`            | Number of training epochs                                | `3` to `10`          |\n",
    "|                  | `per_device_train_batch_size` | Batch size per GPU                                      | `4`, `8`, `16`       |\n",
    "| **Tokenizer**    | `max_length`               | Maximum sequence length after tokenization               | `64`, `128`          |\n",
    "| **Model head**   | Dropout rate               | Dropout before classification layer                      | `0.1`, `0.3`, `0.5`  |\n",
    "\n",
    "### Optimization Strategy\n",
    "\n",
    "I conduct manual grid search and record performance (macro F1, validation loss) for each combination. Best models are selected based on lowest validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccac3c4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
