{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e413d4d",
   "metadata": {},
   "source": [
    "# Curation and creation of data for LLM finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cf3e",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c7d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38c32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd8299",
   "metadata": {},
   "source": [
    "## 1. LLaMA Parameter Optmisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2df90",
   "metadata": {},
   "source": [
    "*Question 1: Which model does the training save? Which performance is it based off of?*\n",
    "\n",
    "`load_best_model_at_end=True,` combined with `eval_strategy=\"epoch\", save_strategy=\"epoch\", save_total_limit=1,` means that \n",
    "\n",
    "- The model is evaluated at the end of each epoch.\n",
    "- Only the best model according to the default metric is retained at the end (save_total_limit=1 prevents clutter).\n",
    "- `Trainer` will automatically reload the best-performing checkpoint at the end based on the evaluation loss (by default).\n",
    "\n",
    "So **the model saved is the one with the lowest validation loss at the end of its epoch**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a80515",
   "metadata": {},
   "source": [
    "*Question 2: Which training/LoRA parameters can be explored to improve performance?*\n",
    "\n",
    "There are two optimisation targets:\n",
    "- LoRA configuration\n",
    "- Training hyperparameters\n",
    "\n",
    "(A) LoRA parameters (from LoraConfig) include\n",
    "- r (e.g. 4 ro 32)\n",
    "- lora_alpha (e.g. 8 to 64)\n",
    "- target modules (e.g q_proj, k_proj, v_proj, o_proj, but also gate_proj, down_proj, up_proj)\n",
    "- lora_dropout (0, or 0.05, 0.1 if overfitting)\n",
    "\n",
    "(B) Training hyperparameters (from TrainingArguments)\n",
    "- learning_rate (e.g. 9e-5 to 2e-4)\n",
    "- per_device_train_batch_size (e.g. 4 to 16)\n",
    "- num_train_epochs\n",
    "- warmup_ratio\n",
    "- lr_scheduler_type\n",
    "- weight_decay\n",
    "- gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b81c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LaTeX table with bolded best row saved to ../results/latex_tables/top5_lora_search.tex\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "log_path = \"../llama_search_runs/search_progress_log.txt\"\n",
    "output_dir = Path(\"../results/latex_tables\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_dir / \"top5_lora_search.tex\"\n",
    "\n",
    "# Read log\n",
    "with open(log_path, \"r\") as f:\n",
    "    log_lines = f.readlines()\n",
    "\n",
    "entries = []\n",
    "for line in log_lines:\n",
    "    run_match = re.search(r\"run_\\d+_r(\\d+)_alpha(\\d+)_drop([0-9.]+)_lr([\\deE.-]+)_bs(\\d+)\", line)\n",
    "    f1_match = re.search(r\"macro_f1: ([0-9.]+)\", line)\n",
    "    loss_match = re.search(r\"val_loss: ([0-9.]+)\", line)\n",
    "    if run_match and f1_match and loss_match:\n",
    "        r, alpha, drop, lr, bs = run_match.groups()\n",
    "        entries.append({\n",
    "            \"LoRA rank ($r$)\": int(r),\n",
    "            \"LoRA alpha\": int(alpha),\n",
    "            \"LoRA dropout\": float(drop),\n",
    "            \"Learning rate\": lr,\n",
    "            \"Batch size\": int(bs),\n",
    "            \"Macro-F1\": float(f1_match.group(1)),\n",
    "            \"Val loss\": float(loss_match.group(1))\n",
    "        })\n",
    "\n",
    "# DataFrame and top 5\n",
    "df = pd.DataFrame(entries)\n",
    "top5 = df.sort_values(\"Macro-F1\", ascending=False).head(5).reset_index(drop=True)\n",
    "\n",
    "# Round values\n",
    "top5[\"LoRA dropout\"] = top5[\"LoRA dropout\"].round(2)\n",
    "top5[\"Macro-F1\"] = top5[\"Macro-F1\"].round(2)\n",
    "top5[\"Val loss\"] = top5[\"Val loss\"].round(2)\n",
    "\n",
    "# Convert all values to string (so LaTeX formatting is valid)\n",
    "top5 = top5.astype(str)\n",
    "\n",
    "# Identify the best row within top5\n",
    "best_idx = top5[\"Macro-F1\"].astype(float).idxmax()\n",
    "\n",
    "# Bold the best row\n",
    "top5.loc[best_idx] = top5.loc[best_idx].apply(lambda x: f\"\\\\textbf{{{x}}}\")\n",
    "\n",
    "# Generate LaTeX\n",
    "latex_table = top5.to_latex(index=False, escape=False)\n",
    "\n",
    "# Wrap in resizebox with caption and label\n",
    "wrapped_latex = f\"\"\"\n",
    "\\\\centering\n",
    "\\\\resizebox{{\\\\linewidth}}{{!}}{{%\n",
    "{latex_table}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Save LaTeX file\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(wrapped_latex)\n",
    "\n",
    "print(f\"✅ LaTeX table with bolded best row saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae86ca3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eecc09a1",
   "metadata": {},
   "source": [
    "## 2. RoBERTa Parameter Optimisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
