{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e413d4d",
   "metadata": {},
   "source": [
    "# Curation and creation of data for LLM finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cf3e",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c7d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38c32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd8299",
   "metadata": {},
   "source": [
    "## 1. LLaMA Parameter Optmisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2df90",
   "metadata": {},
   "source": [
    "*Question 1: Which model does the training save? Which performance is it based off of?*\n",
    "\n",
    "`load_best_model_at_end=True,` combined with `eval_strategy=\"epoch\", save_strategy=\"epoch\", save_total_limit=1,` means that \n",
    "\n",
    "- The model is evaluated at the end of each epoch.\n",
    "- Only the best model according to the default metric is retained at the end (save_total_limit=1 prevents clutter).\n",
    "- `Trainer` will automatically reload the best-performing checkpoint at the end based on the evaluation loss (by default).\n",
    "\n",
    "So **the model saved is the one with the lowest validation loss at the end of its epoch**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a80515",
   "metadata": {},
   "source": [
    "*Question 2: Which training/LoRA parameters can you explore to improve performance?*\n",
    "\n",
    "There are two optimisation targets:\n",
    "- LoRA configuration\n",
    "- Training hyperparameters\n",
    "\n",
    "(A) LoRA parameters (from LoraConfig) include\n",
    "- r (e.g. 4 ro 32)\n",
    "- lora_alpha (e.g. 8 to 64)\n",
    "- target modules (e.g q_proj, k_proj, v_proj, o_proj, but also gate_proj, down_proj, up_proj)\n",
    "- lora_dropout (0, or 0.05, 0.1 if overfitting)\n",
    "\n",
    "(B) Training hyperparameters (from TrainingArguments)\n",
    "- learning_rate (e.g. 9e-5 to 2e-4)\n",
    "- per_device_train_batch_size (e.g. 4 to 16)\n",
    "- num_train_epochs\n",
    "- warmup_ratio\n",
    "- lr_scheduler_type\n",
    "- weight_decay\n",
    "- gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae86ca3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eecc09a1",
   "metadata": {},
   "source": [
    "## 2. RoBERTa Parameter Optimisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
