{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039edfb1",
   "metadata": {},
   "source": [
    "# LLaMA-8b-Instruct Fine-tuning for SDoH Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711eee1",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdff6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdfb7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d208b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # A100 (nvtop Device 0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Use nvtop Device 1 (A100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181fb553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "CACHE_DIR = \"/data/resource/huggingface/hub\"\n",
    "MODEL_OUTPUT_DIR = \"../results/model_training/llama_lora_binary_sdoh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0de19",
   "metadata": {},
   "source": [
    "## 1. Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708ce06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classification.prompt_creation_helpers import create_automated_prompt\n",
    "\n",
    "train_df = pd.read_csv(\"../data/processed/train-test/train_set.csv\")\n",
    "test_df = pd.read_csv(\"../data/processed/train-test/test_set.csv\")\n",
    "\n",
    "def make_prompt(row):\n",
    "    return create_automated_prompt(\n",
    "        sentence=row[\"Sentence\"],\n",
    "        labels=row[\"completion\"],\n",
    "        task_type=\"sdoh_detection\"\n",
    "    )\n",
    "\n",
    "train_df[\"text\"] = train_df.apply(make_prompt, axis=1)\n",
    "test_df[\"text\"] = test_df.apply(make_prompt, axis=1)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\"]])\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"text\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2302844",
   "metadata": {},
   "source": [
    "## 2. Load tokenizer & model in 4-bit & apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d54a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e66ceeb37904338850bc1e67e522c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME, cache_dir=CACHE_DIR, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # LLaMA doesn't have a pad token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08efd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d6db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 0 - NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "print(\"Using device:\", torch.cuda.current_device(), \"-\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80290df",
   "metadata": {},
   "source": [
    "## 3. Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3969788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d79ad9b11d4b62bf9c90322c6ea31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/564 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690a6f2dd8b44a1384f3bf862c3de9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/243 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "    encoding = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()\n",
    "    return encoding\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80c9f9",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691001d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 3\n",
      "torch.cuda.device_count(): 1\n",
      "cuda:0 -> NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"torch.cuda.device_count():\", torch.cuda.device_count())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"cuda:{i} -> {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a3afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3168719/2603611876.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    label_names=[\"labels\"],\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir=os.path.join(MODEL_OUTPUT_DIR, \"logs\"),\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=True,  # or fp16 if not on A100\n",
    "    logging_steps=10,\n",
    "    report_to=[],\n",
    "    run_name=\"llama3_lora_sdoh\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf1b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 04:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.390792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.410100</td>\n",
       "      <td>0.366949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-686dbe83-6daf928f1988de3a4e87dd76;06facb37-ae00-4489-ae4e-95ec7164e009)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct to ask for access. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-686dbeff-111d30222606e2bc302e2b3f;d4023969-e0f7-4264-b9f1-710ff6671c54)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct to ask for access. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=282, training_loss=0.42493336526214653, metrics={'train_runtime': 253.2473, 'train_samples_per_second': 4.454, 'train_steps_per_second': 1.114, 'total_flos': 2.6029803077369856e+16, 'train_loss': 0.42493336526214653, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fefd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-686dbf00-36468b627db01c1e2a14a07e;847bf07d-cc8f-4a20-9a10-2f46ea87ca2a)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct to ask for access. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to ../results/model_training/llama_lora_binary_sdoh\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(f\"✅ Model saved to {MODEL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee85e5a",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ce36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and config\n",
    "MODEL_OUTPUT_DIR = \"../results/llama_lora_sdoh_detection\"\n",
    "TEST_CSV = \"../data/processed/train-test/test_set.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model + tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_OUTPUT_DIR, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Limit to 5 samples\n",
    "df_sample = df.sample(n=5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Generate prompt\n",
    "df_sample[\"prompt\"] = df_sample.apply(\n",
    "    lambda row: create_automated_prompt(\n",
    "        sentence=row[\"Sentence\"],\n",
    "        labels=row[\"completion\"],\n",
    "        task_type=\"sdoh_detection\"\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Generate responses\n",
    "def generate_response(prompt, max_new_tokens=64):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "# Extract just <LIST>...</LIST>\n",
    "def extract_list_output(output_text):\n",
    "    start = output_text.find(\"<LIST>\")\n",
    "    end = output_text.find(\"</LIST>\")\n",
    "    if start != -1 and end != -1:\n",
    "        return output_text[start:end+7]\n",
    "    return \"NO_LIST_FOUND\"\n",
    "\n",
    "# Run generation\n",
    "df_sample[\"generated_completion\"] = df_sample[\"prompt\"].apply(lambda x: extract_list_output(generate_response(x)))\n",
    "\n",
    "# Display results\n",
    "for i, row in df_sample.iterrows():\n",
    "    print(f\"🔢 Example {i+1}\")\n",
    "    print(\"📝 Sentence:\", row[\"Sentence\"])\n",
    "    print(\"✅ True label:\", row[\"completion\"])\n",
    "    print(\"📤 Prompt:\\n\", row[\"prompt\"])\n",
    "    print(\"🤖 Generated:\\n\", row[\"generated_completion\"])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdcf79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e13b21adda45f287c568be0b77a9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   0%|          | 0/243 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   0%|          | 1/243 [00:10<41:29, 10.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   1%|          | 2/243 [00:14<27:52,  6.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   1%|          | 3/243 [00:19<23:27,  5.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   2%|▏         | 4/243 [00:24<21:18,  5.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   2%|▏         | 5/243 [00:28<20:05,  5.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   2%|▏         | 6/243 [00:33<19:23,  4.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   3%|▎         | 7/243 [00:37<18:58,  4.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   3%|▎         | 8/243 [00:42<18:31,  4.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   4%|▎         | 9/243 [00:46<18:16,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   4%|▍         | 10/243 [00:51<18:03,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   5%|▍         | 11/243 [00:56<17:49,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   5%|▍         | 12/243 [01:00<17:44,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   5%|▌         | 13/243 [01:05<17:36,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   6%|▌         | 14/243 [01:10<17:51,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   6%|▌         | 15/243 [01:14<17:39,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   7%|▋         | 16/243 [01:19<17:33,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   7%|▋         | 17/243 [01:24<18:19,  4.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   7%|▋         | 18/243 [01:29<17:47,  4.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   8%|▊         | 19/243 [01:33<17:39,  4.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   8%|▊         | 20/243 [01:38<17:18,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   9%|▊         | 21/243 [01:42<17:03,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   9%|▉         | 22/243 [01:47<17:08,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:   9%|▉         | 23/243 [01:52<17:35,  4.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  10%|▉         | 24/243 [01:57<17:15,  4.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  10%|█         | 25/243 [02:01<17:09,  4.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  11%|█         | 26/243 [02:06<16:47,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  11%|█         | 27/243 [02:10<16:34,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  12%|█▏        | 28/243 [02:15<16:29,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  12%|█▏        | 29/243 [02:20<16:27,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  12%|█▏        | 30/243 [02:24<16:30,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  13%|█▎        | 31/243 [02:29<16:24,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  13%|█▎        | 32/243 [02:34<16:20,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  14%|█▎        | 33/243 [02:38<16:13,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  14%|█▍        | 34/243 [02:43<16:03,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  14%|█▍        | 35/243 [02:48<16:00,  4.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  15%|█▍        | 36/243 [02:52<15:52,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  15%|█▌        | 37/243 [02:57<15:44,  4.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  16%|█▌        | 38/243 [03:01<15:41,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  16%|█▌        | 39/243 [03:06<15:30,  4.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  16%|█▋        | 40/243 [03:10<15:33,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  17%|█▋        | 41/243 [03:15<15:26,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  17%|█▋        | 42/243 [03:19<15:19,  4.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  18%|█▊        | 43/243 [03:24<15:13,  4.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  18%|█▊        | 44/243 [03:29<15:08,  4.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  19%|█▊        | 45/243 [03:33<15:12,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  19%|█▉        | 46/243 [03:38<15:07,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  19%|█▉        | 47/243 [03:42<15:00,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  20%|█▉        | 48/243 [03:47<14:57,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  20%|██        | 49/243 [03:52<14:49,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  21%|██        | 50/243 [03:56<14:44,  4.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  21%|██        | 51/243 [04:01<14:40,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  21%|██▏       | 52/243 [04:05<14:37,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  22%|██▏       | 53/243 [04:10<14:34,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  22%|██▏       | 54/243 [04:15<14:31,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  23%|██▎       | 55/243 [04:19<14:27,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  23%|██▎       | 56/243 [04:24<14:19,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  23%|██▎       | 57/243 [04:28<14:12,  4.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  24%|██▍       | 58/243 [04:33<14:09,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  24%|██▍       | 59/243 [04:38<14:15,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  25%|██▍       | 60/243 [04:42<14:10,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  25%|██▌       | 61/243 [04:47<14:03,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  26%|██▌       | 62/243 [04:52<13:52,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  26%|██▌       | 63/243 [04:56<13:47,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  26%|██▋       | 64/243 [05:01<13:43,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  27%|██▋       | 65/243 [05:05<13:41,  4.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  27%|██▋       | 66/243 [05:10<13:37,  4.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  28%|██▊       | 67/243 [05:15<13:31,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  28%|██▊       | 68/243 [05:19<13:23,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  28%|██▊       | 69/243 [05:24<13:17,  4.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  29%|██▉       | 70/243 [05:28<13:07,  4.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  29%|██▉       | 71/243 [05:33<13:06,  4.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  30%|██▉       | 72/243 [05:37<12:56,  4.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  30%|███       | 73/243 [05:42<12:54,  4.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  30%|███       | 74/243 [05:47<12:55,  4.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  31%|███       | 75/243 [05:51<12:52,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  31%|███▏      | 76/243 [05:56<12:50,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  32%|███▏      | 77/243 [06:01<12:50,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  32%|███▏      | 78/243 [06:05<12:50,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  33%|███▎      | 79/243 [06:10<12:48,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  33%|███▎      | 80/243 [06:15<12:46,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  33%|███▎      | 81/243 [06:19<12:43,  4.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  34%|███▎      | 82/243 [06:24<12:34,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  34%|███▍      | 83/243 [06:29<12:27,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  35%|███▍      | 84/243 [06:33<12:23,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  35%|███▍      | 85/243 [06:38<12:16,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  35%|███▌      | 86/243 [06:43<12:11,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  36%|███▌      | 87/243 [06:47<12:07,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  36%|███▌      | 88/243 [06:52<12:01,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  37%|███▋      | 89/243 [06:57<11:55,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  37%|███▋      | 90/243 [07:01<11:53,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  37%|███▋      | 91/243 [07:06<11:47,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  38%|███▊      | 92/243 [07:11<11:41,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  38%|███▊      | 93/243 [07:15<11:38,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  39%|███▊      | 94/243 [07:20<11:34,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  39%|███▉      | 95/243 [07:25<11:28,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  40%|███▉      | 96/243 [07:29<11:25,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  40%|███▉      | 97/243 [07:34<11:19,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  40%|████      | 98/243 [07:39<11:14,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  41%|████      | 99/243 [07:43<11:11,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  41%|████      | 100/243 [07:48<11:08,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  42%|████▏     | 101/243 [07:53<11:00,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  42%|████▏     | 102/243 [07:57<10:55,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  42%|████▏     | 103/243 [08:02<10:52,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  43%|████▎     | 104/243 [08:06<10:44,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  43%|████▎     | 105/243 [08:11<10:39,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  44%|████▎     | 106/243 [08:16<10:37,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  44%|████▍     | 107/243 [08:20<10:31,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  44%|████▍     | 108/243 [08:25<10:22,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  45%|████▍     | 109/243 [08:30<10:16,  4.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  45%|████▌     | 110/243 [08:34<10:13,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  46%|████▌     | 111/243 [08:39<10:08,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  46%|████▌     | 112/243 [08:43<10:04,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  47%|████▋     | 113/243 [08:48<10:02,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  47%|████▋     | 114/243 [08:53<10:01,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  47%|████▋     | 115/243 [08:58<10:01,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  48%|████▊     | 116/243 [09:02<10:00,  4.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  48%|████▊     | 117/243 [09:07<09:53,  4.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  49%|████▊     | 118/243 [09:12<09:46,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  49%|████▉     | 119/243 [09:16<09:42,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  49%|████▉     | 120/243 [09:21<09:39,  4.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  50%|████▉     | 121/243 [09:26<09:34,  4.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  50%|█████     | 122/243 [09:31<09:31,  4.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  51%|█████     | 123/243 [09:35<09:27,  4.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  51%|█████     | 124/243 [09:40<09:21,  4.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  51%|█████▏    | 125/243 [09:45<09:14,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  52%|█████▏    | 126/243 [09:49<09:09,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  52%|█████▏    | 127/243 [09:54<09:03,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  53%|█████▎    | 128/243 [09:59<08:57,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  53%|█████▎    | 129/243 [10:03<08:52,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  53%|█████▎    | 130/243 [10:08<08:44,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  54%|█████▍    | 131/243 [10:13<08:37,  4.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  54%|█████▍    | 132/243 [10:17<08:35,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  55%|█████▍    | 133/243 [10:22<08:31,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  55%|█████▌    | 134/243 [10:26<08:24,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  56%|█████▌    | 135/243 [10:31<08:25,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  56%|█████▌    | 136/243 [10:36<08:19,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  56%|█████▋    | 137/243 [10:41<08:13,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  57%|█████▋    | 138/243 [10:45<08:06,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  57%|█████▋    | 139/243 [10:50<08:01,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  58%|█████▊    | 140/243 [10:54<07:58,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  58%|█████▊    | 141/243 [10:59<07:53,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  58%|█████▊    | 142/243 [11:04<07:50,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  59%|█████▉    | 143/243 [11:08<07:46,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  59%|█████▉    | 144/243 [11:13<07:40,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  60%|█████▉    | 145/243 [11:18<07:36,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  60%|██████    | 146/243 [11:22<07:33,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  60%|██████    | 147/243 [11:27<07:28,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  61%|██████    | 148/243 [11:32<07:26,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  61%|██████▏   | 149/243 [11:37<07:21,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  62%|██████▏   | 150/243 [11:41<07:14,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  62%|██████▏   | 151/243 [11:46<07:10,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  63%|██████▎   | 152/243 [11:51<07:04,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  63%|██████▎   | 153/243 [11:55<07:00,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  63%|██████▎   | 154/243 [12:00<06:56,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  64%|██████▍   | 155/243 [12:05<06:51,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  64%|██████▍   | 156/243 [12:09<06:44,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  65%|██████▍   | 157/243 [12:14<06:40,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  65%|██████▌   | 158/243 [12:19<06:38,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  65%|██████▌   | 159/243 [12:23<06:33,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  66%|██████▌   | 160/243 [12:28<06:30,  4.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  66%|██████▋   | 161/243 [12:33<06:27,  4.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  67%|██████▋   | 162/243 [12:37<06:20,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  67%|██████▋   | 163/243 [12:42<06:14,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  67%|██████▋   | 164/243 [12:47<06:10,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  68%|██████▊   | 165/243 [12:51<06:03,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  68%|██████▊   | 166/243 [12:56<05:59,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  69%|██████▊   | 167/243 [13:01<05:57,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  69%|██████▉   | 168/243 [13:05<05:51,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  70%|██████▉   | 169/243 [13:10<05:45,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  70%|██████▉   | 170/243 [13:15<05:40,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  70%|███████   | 171/243 [13:19<05:35,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  71%|███████   | 172/243 [13:24<05:29,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  71%|███████   | 173/243 [13:29<05:24,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  72%|███████▏  | 174/243 [13:33<05:20,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  72%|███████▏  | 175/243 [13:38<05:14,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  72%|███████▏  | 176/243 [13:42<05:09,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  73%|███████▎  | 177/243 [13:47<05:04,  4.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  73%|███████▎  | 178/243 [13:52<05:00,  4.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  74%|███████▎  | 179/243 [13:56<04:54,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  74%|███████▍  | 180/243 [14:01<04:51,  4.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  74%|███████▍  | 181/243 [14:06<04:47,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  75%|███████▍  | 182/243 [14:10<04:42,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  75%|███████▌  | 183/243 [14:15<04:36,  4.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  76%|███████▌  | 184/243 [14:20<04:33,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  76%|███████▌  | 185/243 [14:24<04:28,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  77%|███████▋  | 186/243 [14:29<04:24,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  77%|███████▋  | 187/243 [14:33<04:21,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  77%|███████▋  | 188/243 [14:38<04:14,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  78%|███████▊  | 189/243 [14:43<04:12,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  78%|███████▊  | 190/243 [14:48<04:08,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  79%|███████▊  | 191/243 [14:52<04:03,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  79%|███████▉  | 192/243 [14:57<03:57,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  79%|███████▉  | 193/243 [15:01<03:53,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  80%|███████▉  | 194/243 [15:06<03:49,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  80%|████████  | 195/243 [15:11<03:44,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  81%|████████  | 196/243 [15:16<03:40,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  81%|████████  | 197/243 [15:21<03:43,  4.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  81%|████████▏ | 198/243 [15:26<03:36,  4.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  82%|████████▏ | 199/243 [15:30<03:29,  4.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  82%|████████▏ | 200/243 [15:36<03:34,  4.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  83%|████████▎ | 201/243 [15:40<03:26,  4.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  83%|████████▎ | 202/243 [15:45<03:19,  4.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  84%|████████▎ | 203/243 [15:50<03:12,  4.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  84%|████████▍ | 204/243 [15:55<03:06,  4.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  84%|████████▍ | 205/243 [15:59<03:00,  4.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  85%|████████▍ | 206/243 [16:04<02:55,  4.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  85%|████████▌ | 207/243 [16:09<02:56,  4.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  86%|████████▌ | 208/243 [16:15<02:56,  5.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  86%|████████▌ | 209/243 [16:19<02:48,  4.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  86%|████████▋ | 210/243 [16:24<02:42,  4.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  87%|████████▋ | 211/243 [16:30<02:41,  5.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  87%|████████▋ | 212/243 [16:34<02:33,  4.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  88%|████████▊ | 213/243 [16:39<02:25,  4.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  88%|████████▊ | 214/243 [16:44<02:22,  4.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  88%|████████▊ | 215/243 [16:49<02:19,  4.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  89%|████████▉ | 216/243 [16:54<02:10,  4.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  89%|████████▉ | 217/243 [16:58<02:03,  4.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  90%|████████▉ | 218/243 [17:03<01:58,  4.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  90%|█████████ | 219/243 [17:07<01:52,  4.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  91%|█████████ | 220/243 [17:12<01:47,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  91%|█████████ | 221/243 [17:17<01:42,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  91%|█████████▏| 222/243 [17:21<01:37,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  92%|█████████▏| 223/243 [17:26<01:33,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  92%|█████████▏| 224/243 [17:31<01:28,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  93%|█████████▎| 225/243 [17:35<01:23,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  93%|█████████▎| 226/243 [17:40<01:18,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  93%|█████████▎| 227/243 [17:45<01:14,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  94%|█████████▍| 228/243 [17:49<01:09,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  94%|█████████▍| 229/243 [17:54<01:04,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  95%|█████████▍| 230/243 [17:59<01:00,  4.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  95%|█████████▌| 231/243 [18:03<00:56,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  95%|█████████▌| 232/243 [18:08<00:51,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  96%|█████████▌| 233/243 [18:13<00:46,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  96%|█████████▋| 234/243 [18:17<00:42,  4.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  97%|█████████▋| 235/243 [18:22<00:37,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  97%|█████████▋| 236/243 [18:27<00:32,  4.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  98%|█████████▊| 237/243 [18:31<00:28,  4.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  98%|█████████▊| 238/243 [18:36<00:23,  4.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  98%|█████████▊| 239/243 [18:41<00:18,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  99%|█████████▉| 240/243 [18:45<00:13,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions:  99%|█████████▉| 241/243 [18:50<00:09,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions: 100%|█████████▉| 242/243 [18:55<00:04,  4.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating predictions: 100%|██████████| 243/243 [18:59<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report (binary presence):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NoSDoH       0.41      1.00      0.58        99\n",
      "    Any SDoH       0.00      0.00      0.00       144\n",
      "\n",
      "    accuracy                           0.41       243\n",
      "   macro avg       0.20      0.50      0.29       243\n",
      "weighted avg       0.17      0.41      0.24       243\n",
      "\n",
      "\n",
      "✅ Predictions saved to ../results/model_training/llama_lora_binary_sdoh/eval_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda/envs/keble8263-sdoh-extraction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import TextStreamer, BitsAndBytesConfig\n",
    "\n",
    "# ================\n",
    "# 🔧 Setup\n",
    "# ================\n",
    "MODEL_OUTPUT_DIR = \"../results/model_training/llama_lora_binary_sdoh\"\n",
    "CACHE_DIR = \"/data/resource/huggingface/hub\"\n",
    "TEST_CSV = \"../data/processed/train-test/test_set.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# ================\n",
    "# 📥 Load model in 4-bit\n",
    "# ================\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_OUTPUT_DIR, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_OUTPUT_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# ================\n",
    "# 📊 Load test set\n",
    "# ================\n",
    "df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# ================\n",
    "# 📜 Generate prompts\n",
    "# ================\n",
    "def make_prompt(sentence, labels=None):\n",
    "    return create_automated_prompt(sentence=sentence, labels=labels, task_type=\"sdoh_detection\")\n",
    "\n",
    "df[\"prompt\"] = df.apply(lambda row: make_prompt(row[\"Sentence\"], row[\"completion\"]), axis=1)\n",
    "\n",
    "# ================\n",
    "# 🔮 Generate predictions\n",
    "# ================\n",
    "def generate_response(prompt, max_new_tokens=64):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "def extract_list_output(output_text):\n",
    "    # Extracts content between <LIST>...</LIST>\n",
    "    start = output_text.find(\"<LIST>\")\n",
    "    end = output_text.find(\"</LIST>\")\n",
    "    if start != -1 and end != -1:\n",
    "        return output_text[start:end+7]\n",
    "    return \"NO_LIST_FOUND\"\n",
    "\n",
    "# tqdm progress bar for batch generation\n",
    "outputs = []\n",
    "for prompt in tqdm(df[\"prompt\"], desc=\"Generating predictions\"):\n",
    "    full_output = generate_response(prompt)\n",
    "    outputs.append(extract_list_output(full_output))\n",
    "\n",
    "df[\"generated_completion\"] = outputs\n",
    "\n",
    "# ================\n",
    "# 🧮 Evaluation (basic)\n",
    "# ================\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = df[\"completion\"].apply(lambda x: \"NoSDoH\" if \"NoSDoH\" in x else \"AnySDoH\")\n",
    "y_pred = df[\"generated_completion\"].apply(lambda x: \"NoSDoH\" if \"NoSDoH\" in x else \"AnySDoH\")\n",
    "\n",
    "print(\"\\n📊 Classification Report (binary presence):\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"NoSDoH\", \"Any SDoH\"]))\n",
    "\n",
    "# ================\n",
    "# 💾 Save results\n",
    "# ================\n",
    "df[[\"Sentence\", \"completion\", \"generated_completion\"]].to_csv(\n",
    "    os.path.join(MODEL_OUTPUT_DIR, \"eval_predictions.csv\"),\n",
    "    index=False\n",
    ")\n",
    "print(f\"\\n✅ Predictions saved to {MODEL_OUTPUT_DIR}/eval_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19fda1e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
