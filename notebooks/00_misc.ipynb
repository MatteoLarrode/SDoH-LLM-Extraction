{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ceccc6",
   "metadata": {},
   "source": [
    "# Miscellaneous code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850d24f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7730f6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672a4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae47dd1",
   "metadata": {},
   "source": [
    "## Aesthetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour palette\n",
    "# From https://brand.ifrc.org/ifrc-brand-system/basics/colour\n",
    "colour_palette = {\n",
    "    'ifrc_red': '#EE2435',\n",
    "    'ifrc_darkblue': '#011E41',\n",
    "    'dark_green': '#009775',\n",
    "    'medium_green': '#00AB84',\n",
    "    'light_green': '#47D7AC',\n",
    "    'medium_blue': '#8DCDE2',\n",
    "    'light_blue': '#CCf5FC',\n",
    "    'medium_orange': '#FF8200',\n",
    "    'light_orange': '#FFB25B',\n",
    "    'medium_purple': '#512D6D',\n",
    "    'light_purple': '#958DBE',\n",
    "    'grey': '#A7A8AA',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0058643",
   "metadata": {},
   "source": [
    "## Background on brains & cached models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b6bc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cached models:\n",
      "  CohereForAI/aya-23-35B\n",
      "  CohereForAI/aya-23-8B\n",
      "  CohereForAI/aya-vision-8b\n",
      "  HuggingFaceTB/SmolLM-135M-Instruct\n",
      "  LLaMAX/LLaMAX3-8B-Alpaca\n",
      "  Qwen/Qwen1.5-4B\n",
      "  Qwen/Qwen2-7B\n",
      "  Qwen/Qwen2.5-1.5B\n",
      "  Qwen/Qwen2.5-3B\n",
      "  Qwen/Qwen2.5-72B-Instruct\n",
      "  Qwen/Qwen2.5-7B\n",
      "  Qwen/Qwen2.5-7B-Instruct\n",
      "  Qwen/Qwen2.5-7B-instruct\n",
      "  Qwen/Qwen2.5-VL-7B-Instruct\n",
      "  Qwen/Qwen3-0.6B\n",
      "  Qwen/Qwen3-8B\n",
      "  Unbabel/wmt20-comet-qe-da\n",
      "  Unbabel/wmt22-comet-da\n",
      "  bert-base-uncased\n",
      "  bert-large-uncased\n",
      "  cardiffnlp/twitter-roberta-base-sentiment\n",
      "  cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "  clairebarale/refugee_cases_ner\n",
      "  cross-encoder/nli-deberta-v3-large\n",
      "  cross-encoder/stsb-roberta-base\n",
      "  cross-encoder/stsb-roberta-large\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Llama-70B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "  facebook/nllb-200-3.3B\n",
      "  facebook/nllb-200-distilled-1.3B\n",
      "  facebook/nllb-200-distilled-600M\n",
      "  google/gemma-3-1b-it\n",
      "  google/gemma-3-27b-it\n",
      "  google/gemma-3-27b-it-qat-q4_0-gguf\n",
      "  gpt2\n",
      "  gpt2-medium\n",
      "  gpt2-xl\n",
      "  hfl/chinese-bert-wwm\n",
      "  hfl/chinese-electra-180g-small-discriminator\n",
      "  hfl/chinese-legal-electra-base-discriminator\n",
      "  hfl/chinese-legal-electra-small-discriminator\n",
      "  hfl/chinese-roberta-wwm-ext\n",
      "  hfl/chinese-roberta-wwm-ext-large\n",
      "  jxm/gtr__nq__32\n",
      "  jxm/gtr__nq__32__correct\n",
      "  meta-llama/Llama-2-7b-chat-hf\n",
      "  meta-llama/Llama-2-7b-hf\n",
      "  meta-llama/Llama-3.1-70B-Instruct\n",
      "  meta-llama/Llama-3.1-8B\n",
      "  meta-llama/Llama-3.1-8B-Instruct\n",
      "  meta-llama/Llama-3.2-11B-Vision-Instruct\n",
      "  meta-llama/Llama-3.3-70B-Instruct\n",
      "  meta-llama/Llama-4-Scout-17B-16E\n",
      "  meta-llama/Meta-Llama-3-70B-Instruct\n",
      "  meta-llama/Meta-Llama-3-8B\n",
      "  meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "  microsoft/Phi-3-mini-4k-instruct\n",
      "  microsoft/Phi-3.5-vision-instruct\n",
      "  microsoft/Phi-4-mini-instruct\n",
      "  microsoft/Phi-4-multimodal-instruct\n",
      "  microsoft/deberta-large-mnli\n",
      "  microsoft/deberta-v3-base\n",
      "  microsoft/deberta-xlarge\n",
      "  microsoft/deberta-xlarge-mnli\n",
      "  mistral-community/pixtral-12b\n",
      "  mistralai/Mistral-7B-Instruct-v0.2\n",
      "  mistralai/Mistral-7B-Instruct-v0.3\n",
      "  mistralai/Mistral-7B-v0.1\n",
      "  mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "  mosaicml/mpt-7b-chat\n",
      "  nlpaueb/legal-bert-base-uncased\n",
      "  nvidia/Llama-3.1-Nemotron-Nano-8B-v1\n",
      "  openai-community/gpt2\n",
      "  openai-community/gpt2-large\n",
      "  openai-community/gpt2-medium\n",
      "  openai-community/gpt2-xl\n",
      "  openai/whisper-large-v3-turbo\n",
      "  openbmb/MiniCPM-o-2_6\n",
      "  roberta-base\n",
      "  roberta-large\n",
      "  saibo/legal-roberta-base\n",
      "  sentence-transformers/LaBSE\n",
      "  sentence-transformers/all-MPNet-base-v2\n",
      "  sentence-transformers/all-MiniLM-L6-v2\n",
      "  sentence-transformers/all-mpnet-base-v2\n",
      "  sentence-transformers/gtr-t5-base\n",
      "  sentence-transformers/msmarco-bert-co-condensor\n",
      "  sentence-transformers/paraphrase-distilroberta-base-v2\n",
      "  sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "  sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "  shibing624/text2vec-base-chinese\n",
      "  t5-base\n",
      "  t5-large\n",
      "  t5-small\n",
      "  unsloth/llama-3-8b-bnb-4bit\n",
      "  unsloth/meta-llama-3.1-8b-instruct-bnb-4bit\n",
      "  unslothai/2\n",
      "  unslothai/4\n",
      "  unslothai/other\n",
      "  unslothai/repeat\n",
      "  unslothai/vram-48\n",
      "  vahidthegreat/StanceAware-SBERT\n",
      "  xlm-roberta-base\n",
      "  xlm-roberta-large\n"
     ]
    }
   ],
   "source": [
    "# Check models\n",
    "# What models are available\n",
    "cache_dir = \"/data/resource/huggingface/hub\"\n",
    "available_models = []\n",
    "\n",
    "if os.path.exists(cache_dir):\n",
    "    for item in os.listdir(cache_dir):\n",
    "        if item.startswith(\"models--\"):\n",
    "            # Convert models--org--name to org/name format\n",
    "            model_name = item.replace(\"models--\", \"\").replace(\"--\", \"/\")\n",
    "            available_models.append(model_name)\n",
    "\n",
    "print(\"Available cached models:\")\n",
    "for model in sorted(available_models):\n",
    "    print(f\"  {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e5ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  4 CUDA device(s) detected:\n",
      "\n",
      "Device 0: NVIDIA L40S\n",
      "Device 1: NVIDIA L40S\n",
      "Device 2: NVIDIA A100 80GB PCIe\n",
      "Device 3: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"ðŸ§  {torch.cuda.device_count()} CUDA device(s) detected:\\n\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2368273",
   "metadata": {},
   "source": [
    "## Checking prompts and training data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.llama.multilabel_direct.prepare_dataset import prepare_multilabel_dataset, prepare_multilabel_dataset_infer\n",
    "\n",
    "val_prepared = prepare_multilabel_dataset(\"../data/processed/train-test/val_set.csv\")\n",
    "val_prepared_df = val_prepared.to_pandas()\n",
    "\n",
    "test_prepared_df = prepare_multilabel_dataset_infer(\"../data/processed/train-test/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3980a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.llama.multilabel_direct_adverse.prepare_dataset import prepare_adverse_only_dataset, prepare_adverse_only_dataset_infer\n",
    "\n",
    "val_prepared_adverse = prepare_adverse_only_dataset(\"../data/processed/train-test/val_set.csv\")\n",
    "val_prepared_adverse_df = val_prepared_adverse.to_pandas()\n",
    "\n",
    "test_prepared_adverse_df = prepare_adverse_only_dataset_infer(\"../data/processed/train-test/test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6406f",
   "metadata": {},
   "source": [
    "## Two-step pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.multistep.two_step_pipeline import run_two_step_pipeline\n",
    "\n",
    "run_two_step_pipeline(\n",
    "        test_data_file=\"../data/processed/train-test/test_set.csv\",\n",
    "        roberta_model_dir=\"../results/model_training/roberta_binary_sdoh/roberta-base_bs16_lr9e-05_20250709_170452/checkpoint-24\",\n",
    "        llama_model_dir=\"../results/model_training/llama_lora_multi_label_full/Llama-3.1-8B-Instruct_bs8_lr9e-05_epochs6_20250710_164937\",\n",
    "        pos_weight=1.5251,\n",
    "        output_file=\"../results/multistep/two_step_predictions.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea40ae",
   "metadata": {},
   "source": [
    "### Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9187313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label_pair",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label_string",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "completion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "binary_label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "399af853-939a-487b-9f76-c92b9ac2865a",
       "rows": [
        [
         "0",
         "She is able to sit out for XXXX hours between care calls .",
         "['NoSDoH']",
         "NoSDoH",
         "<LIST>NoSDoH</LIST>",
         "0"
        ],
        [
         "1",
         "He is currently treated with Sinemet and Ropinirole .",
         "['NoSDoH']",
         "NoSDoH",
         "<LIST>NoSDoH</LIST>",
         "0"
        ],
        [
         "2",
         "Marker on Essex Wellbeing Record that she was aggressive / violent towards Community Agents .",
         "['NoSDoH']",
         "NoSDoH",
         "<LIST>NoSDoH</LIST>",
         "0"
        ],
        [
         "3",
         "She needs help with food , toiletry and some cash .",
         "['Finances-Adverse', 'FoodAccess-Adverse']",
         "Finances-Adverse|FoodAccess-Adverse",
         "<LIST>Finances-Adverse, FoodAccess-Adverse</LIST>",
         "1"
        ],
        [
         "4",
         "support to find a cleaning service in community + welfare checks",
         "['Housing-Adverse']",
         "Housing-Adverse",
         "<LIST>Housing-Adverse</LIST>",
         "1"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>label_pair</th>\n",
       "      <th>label_string</th>\n",
       "      <th>completion</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She is able to sit out for XXXX hours between ...</td>\n",
       "      <td>['NoSDoH']</td>\n",
       "      <td>NoSDoH</td>\n",
       "      <td>&lt;LIST&gt;NoSDoH&lt;/LIST&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is currently treated with Sinemet and Ropin...</td>\n",
       "      <td>['NoSDoH']</td>\n",
       "      <td>NoSDoH</td>\n",
       "      <td>&lt;LIST&gt;NoSDoH&lt;/LIST&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marker on Essex Wellbeing Record that she was ...</td>\n",
       "      <td>['NoSDoH']</td>\n",
       "      <td>NoSDoH</td>\n",
       "      <td>&lt;LIST&gt;NoSDoH&lt;/LIST&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She needs help with food , toiletry and some c...</td>\n",
       "      <td>['Finances-Adverse', 'FoodAccess-Adverse']</td>\n",
       "      <td>Finances-Adverse|FoodAccess-Adverse</td>\n",
       "      <td>&lt;LIST&gt;Finances-Adverse, FoodAccess-Adverse&lt;/LIST&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>support to find a cleaning service in communit...</td>\n",
       "      <td>['Housing-Adverse']</td>\n",
       "      <td>Housing-Adverse</td>\n",
       "      <td>&lt;LIST&gt;Housing-Adverse&lt;/LIST&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  She is able to sit out for XXXX hours between ...   \n",
       "1  He is currently treated with Sinemet and Ropin...   \n",
       "2  Marker on Essex Wellbeing Record that she was ...   \n",
       "3  She needs help with food , toiletry and some c...   \n",
       "4  support to find a cleaning service in communit...   \n",
       "\n",
       "                                   label_pair  \\\n",
       "0                                  ['NoSDoH']   \n",
       "1                                  ['NoSDoH']   \n",
       "2                                  ['NoSDoH']   \n",
       "3  ['Finances-Adverse', 'FoodAccess-Adverse']   \n",
       "4                         ['Housing-Adverse']   \n",
       "\n",
       "                          label_string  \\\n",
       "0                               NoSDoH   \n",
       "1                               NoSDoH   \n",
       "2                               NoSDoH   \n",
       "3  Finances-Adverse|FoodAccess-Adverse   \n",
       "4                      Housing-Adverse   \n",
       "\n",
       "                                          completion  binary_label  \n",
       "0                                <LIST>NoSDoH</LIST>             0  \n",
       "1                                <LIST>NoSDoH</LIST>             0  \n",
       "2                                <LIST>NoSDoH</LIST>             0  \n",
       "3  <LIST>Finances-Adverse, FoodAccess-Adverse</LIST>             1  \n",
       "4                       <LIST>Housing-Adverse</LIST>             1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: Setup + Read Data\n",
    "import os\n",
    "import pandas as pd\n",
    "from scripts.roberta.dataset import is_sdoh_label\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "test_path = \"../data/processed/train-test/test_set.csv\"\n",
    "roberta_model_dir = \"../results/model_training/roberta_binary_sdoh/roberta-base_bs16_lr9e-05_20250709_170452/checkpoint-24\"\n",
    "llama_model_dir = \"../results/model_training/llama_lora_multi_label_full/Llama-3.1-8B-Instruct_bs8_lr9e-05_epochs6_20250710_164937\"\n",
    "pos_weight = 1.5251\n",
    "\n",
    "df = pd.read_csv(test_path)\n",
    "df[\"binary_label\"] = df[\"completion\"].apply(is_sdoh_label)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59cbc7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1548339/3982882224.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, tokenizer=tokenizer)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 2: Load RoBERTa and Predict\n",
    "from transformers import RobertaTokenizer, RobertaConfig, Trainer\n",
    "from scripts.roberta.dataset import BinarySDoHDataset\n",
    "from scripts.roberta.model import RobertaBinaryClassifierWithWeight\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "config = RobertaConfig.from_pretrained(roberta_model_dir)\n",
    "\n",
    "model = RobertaBinaryClassifierWithWeight.from_pretrained(\n",
    "    roberta_model_dir,\n",
    "    config=config,\n",
    "    pos_weight=pos_weight\n",
    ")\n",
    "\n",
    "dataset = BinarySDoHDataset(df, tokenizer)\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "outputs = trainer.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131ed96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "completion",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "roberta_pred_sdoh",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "roberta_prob_sdoh",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "ref": "f95a2e4d-d278-474b-a3f4-18a34b556102",
       "rows": [
        [
         "0",
         "She is able to sit out for XXXX hours between care calls .",
         "<LIST>NoSDoH</LIST>",
         "0",
         "0.4510482"
        ],
        [
         "1",
         "He is currently treated with Sinemet and Ropinirole .",
         "<LIST>NoSDoH</LIST>",
         "0",
         "0.044357546"
        ],
        [
         "2",
         "Marker on Essex Wellbeing Record that she was aggressive / violent towards Community Agents .",
         "<LIST>NoSDoH</LIST>",
         "1",
         "0.6065941"
        ],
        [
         "3",
         "She needs help with food , toiletry and some cash .",
         "<LIST>Finances-Adverse, FoodAccess-Adverse</LIST>",
         "1",
         "0.9689461"
        ],
        [
         "4",
         "support to find a cleaning service in community + welfare checks",
         "<LIST>Housing-Adverse</LIST>",
         "1",
         "0.9213233"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>completion</th>\n",
       "      <th>roberta_pred_sdoh</th>\n",
       "      <th>roberta_prob_sdoh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She is able to sit out for XXXX hours between ...</td>\n",
       "      <td>&lt;LIST&gt;NoSDoH&lt;/LIST&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0.451048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is currently treated with Sinemet and Ropin...</td>\n",
       "      <td>&lt;LIST&gt;NoSDoH&lt;/LIST&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marker on Essex Wellbeing Record that she was ...</td>\n",
       "      <td>&lt;LIST&gt;NoSDoH&lt;/LIST&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She needs help with food , toiletry and some c...</td>\n",
       "      <td>&lt;LIST&gt;Finances-Adverse, FoodAccess-Adverse&lt;/LIST&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0.968946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>support to find a cleaning service in communit...</td>\n",
       "      <td>&lt;LIST&gt;Housing-Adverse&lt;/LIST&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  She is able to sit out for XXXX hours between ...   \n",
       "1  He is currently treated with Sinemet and Ropin...   \n",
       "2  Marker on Essex Wellbeing Record that she was ...   \n",
       "3  She needs help with food , toiletry and some c...   \n",
       "4  support to find a cleaning service in communit...   \n",
       "\n",
       "                                          completion  roberta_pred_sdoh  \\\n",
       "0                                <LIST>NoSDoH</LIST>                  0   \n",
       "1                                <LIST>NoSDoH</LIST>                  0   \n",
       "2                                <LIST>NoSDoH</LIST>                  1   \n",
       "3  <LIST>Finances-Adverse, FoodAccess-Adverse</LIST>                  1   \n",
       "4                       <LIST>Housing-Adverse</LIST>                  1   \n",
       "\n",
       "   roberta_prob_sdoh  \n",
       "0           0.451048  \n",
       "1           0.044358  \n",
       "2           0.606594  \n",
       "3           0.968946  \n",
       "4           0.921323  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 3: Add RoBERTa Predictions\n",
    "probs = torch.sigmoid(torch.tensor(outputs.predictions)).numpy().flatten()\n",
    "y_pred = (probs > 0.5).astype(int)\n",
    "\n",
    "df[\"roberta_prob_sdoh\"] = probs\n",
    "df[\"roberta_pred_sdoh\"] = y_pred\n",
    "df_roberta = df[[\"Sentence\", \"completion\", \"roberta_pred_sdoh\", \"roberta_prob_sdoh\"]].copy()\n",
    "df_roberta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7ac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 5: Prepare Subset for LLaMA\n",
    "df_flagged = df_roberta[df_roberta[\"roberta_pred_sdoh\"] == 1].copy()\n",
    "df_flagged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7f890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1884ceeeab14dc8b3d548ec84efdca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Loading LoRA adapters from: ../results/model_training/llama_lora_multi_label_full/Llama-3.1-8B-Instruct_bs8_lr9e-05_epochs6_20250710_164937\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Run LLaMA on Flagged Sentences\n",
    "from scripts.llama.shared_utils.model import load_lora_llama\n",
    "from scripts.llama.multilabel_direct.prepare_dataset import prepare_multilabel_dataset_infer\n",
    "from tqdm import tqdm\n",
    "\n",
    "model, tokenizer = load_lora_llama(\n",
    "    base_model_path=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    adapter_path=llama_model_dir,\n",
    "    cache_dir=\"/data/resource/huggingface/hub\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "df_prompted = prepare_multilabel_dataset_infer(df_flagged.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdde4090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [01:13<00:00,  1.54it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "generated_completion",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6160f806-124c-442e-8c59-6996b4acfe8d",
       "rows": [
        [
         "2",
         "Marker on Essex Wellbeing Record that she was aggressive / violent towards Community Agents .",
         "<LIST>NoSDoH</LIST>"
        ],
        [
         "3",
         "She needs help with food , toiletry and some cash .",
         "<LIST>FoodAccess, Finances</LIST>"
        ],
        [
         "4",
         "support to find a cleaning service in community + welfare checks",
         "<LIST>Housing, Loneliness</LIST>"
        ],
        [
         "5",
         "PERSON has hearing aids & struggles with phone calls .",
         "<LIST>NoSDoH</LIST>"
        ],
        [
         "6",
         "The patient requires the internet to complete shopping so may need assistance to complete this whilst awaiting internet to be fixed .",
         "<LIST>DigitalInclusion</LIST>"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>generated_completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marker on Essex Wellbeing Record that she was ...</td>\n",
       "      <td>&lt;LIST&gt;NoSDoH&lt;/LIST&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She needs help with food , toiletry and some c...</td>\n",
       "      <td>&lt;LIST&gt;FoodAccess, Finances&lt;/LIST&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>support to find a cleaning service in communit...</td>\n",
       "      <td>&lt;LIST&gt;Housing, Loneliness&lt;/LIST&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PERSON has hearing aids &amp; struggles with phone...</td>\n",
       "      <td>&lt;LIST&gt;NoSDoH&lt;/LIST&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The patient requires the internet to complete ...</td>\n",
       "      <td>&lt;LIST&gt;DigitalInclusion&lt;/LIST&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "2  Marker on Essex Wellbeing Record that she was ...   \n",
       "3  She needs help with food , toiletry and some c...   \n",
       "4  support to find a cleaning service in communit...   \n",
       "5  PERSON has hearing aids & struggles with phone...   \n",
       "6  The patient requires the internet to complete ...   \n",
       "\n",
       "                generated_completion  \n",
       "2                <LIST>NoSDoH</LIST>  \n",
       "3  <LIST>FoodAccess, Finances</LIST>  \n",
       "4   <LIST>Housing, Loneliness</LIST>  \n",
       "5                <LIST>NoSDoH</LIST>  \n",
       "6      <LIST>DigitalInclusion</LIST>  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 7: Generate Predictions\n",
    "def extract_list_output(text):\n",
    "    start, end = text.find(\"<LIST>\"), text.find(\"</LIST>\")\n",
    "    return text[start:end+7] if start != -1 and end != -1 else \"NO_LIST_FOUND\"\n",
    "\n",
    "def generate_response(prompt):\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    decoded = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
    "    return decoded.strip()\n",
    "\n",
    "predictions = []\n",
    "for prompt in tqdm(df_prompted[\"prompt\"]):\n",
    "    output = generate_response(prompt)\n",
    "    predictions.append(extract_list_output(output))\n",
    "\n",
    "df_prompted[\"generated_completion\"] = predictions\n",
    "df_llama = df_prompted[[\"Sentence\", \"generated_completion\"]]\n",
    "df_llama.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Merge and Create Final Predictions\n",
    "df_final = df_roberta.merge(df_llama, on=\"Sentence\", how=\"left\")\n",
    "df_final[\"final_prediction\"] = df_final.apply(\n",
    "    lambda row: row[\"generated_completion\"] if row[\"roberta_pred_sdoh\"] == 1 else \"<LIST>NoSDoH</LIST>\",\n",
    "    axis=1\n",
    ")\n",
    "df_final[[\"Sentence\", \"completion\", \"final_prediction\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31438d9",
   "metadata": {},
   "source": [
    "## Create toy referrals dataset for proof-of-concept code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e908a166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved synthetic dataset to ../data/processed/brc-cleaned/toy_referrals.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_path = \"../data/processed/brc-cleaned\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "output_file = os.path.join(output_path, \"toy_referrals.csv\")\n",
    "\n",
    "# Initialize faker\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define synthetic SDoH phrases\n",
    "sdoh_sentences = [\n",
    "    \"She is struggling with paying her rent and buying food.\",\n",
    "    \"He feels very isolated and does not leave his home.\",\n",
    "    \"The client mentioned she needs help finding a job.\",\n",
    "    \"He is unable to use digital services on his own.\",\n",
    "    \"She cannot understand written English very well.\",\n",
    "    \"He has no money for transportation to the food bank.\",\n",
    "    \"The client reported sleeping on a friend's couch.\",\n",
    "    \"She is fluent in English and volunteers regularly.\",\n",
    "    \"He owns a home and has a stable income.\",\n",
    "    \"The client is digitally literate and employed full-time.\",\n",
    "    \"She receives universal credit and struggles with bills.\",\n",
    "    \"He asked for help accessing community food support.\"\n",
    "]\n",
    "\n",
    "# Generate 20 synthetic referrals\n",
    "rows = []\n",
    "for i in range(20):\n",
    "    num_sents = random.randint(1, min(4, len(sdoh_sentences)))  # prevent sampling too many\n",
    "    note = \" \".join(random.sample(sdoh_sentences, k=num_sents))  # ensure unique sentences per note\n",
    "\n",
    "    row = {\n",
    "        \"Area\": fake.city(),\n",
    "        \"Scheme\": fake.word(),\n",
    "        \"Case Reference\": fake.uuid4(),\n",
    "        \"Assessment Result\": random.choice([\"Needs Met\", \"Needs Unmet\", \"Ongoing\"]),\n",
    "        \"Case Status\": random.choice([\"Open\", \"Closed\", \"Pending\"]),\n",
    "        \"Referral Date/Time\": fake.date_time_this_year().isoformat(),\n",
    "        \"End Date Case\": fake.date_this_year().isoformat(),\n",
    "        \"Has Disability\": random.choice([\"Yes\", \"No\"]),\n",
    "        \"Has Risk\": random.choice([\"Yes\", \"No\"]),\n",
    "        \"Risk Type\": random.choice([\"Mental Health\", \"Domestic\", \"Mobility\", None]),\n",
    "        \"Unique Case\": fake.uuid4(),\n",
    "        \"IMD_decile\": random.randint(1, 10),\n",
    "        \"Country\": random.choice([\"England\", \"Wales\", \"Scotland\"]),\n",
    "        \"Age\": random.randint(18, 90),\n",
    "        \"Gender\": random.choice([\"Male\", \"Female\", \"Other\"]),\n",
    "        \"Ethnicity\": random.choice([\"White\", \"Black\", \"Asian\", \"Mixed\", \"Other\"]),\n",
    "        \"Disability\": random.choice([\"None\", \"Hearing\", \"Visual\", \"Mobility\"]),\n",
    "        \"Living Arrangements\": random.choice([\"Alone\", \"With family\", \"With partner\"]),\n",
    "        \"Referral Notes (depersonalised)\": note,\n",
    "        \"case_ref\": f\"toy_{i:04d}\",\n",
    "        \"num_observations\": random.randint(1, 10),\n",
    "        \"date_range_start\": fake.date_this_year().isoformat(),\n",
    "        \"date_range_end\": fake.date_this_year().isoformat(),\n",
    "        \"date_range_days\": random.randint(1, 120),\n",
    "        \"referral_date\": fake.date_this_year().isoformat()\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Saved synthetic dataset to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c53687",
   "metadata": {},
   "source": [
    "## Decompostion of the inference over all referrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f2a2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Use nvtop Device 1 (A100)\n",
    "\n",
    "from scripts.multistep_adverse.two_step_pipeline import run_two_step_pipeline\n",
    "\n",
    "def sentence_splitter(text):\n",
    "    \"\"\"\n",
    "    Safely split a note into non-empty sentences. Returns [] if text is not a string.\n",
    "    \"\"\"\n",
    "    return [s.strip() for s in text.split('.') if isinstance(s, str) and s.strip()]\n",
    "\n",
    "def prepare_batched_csvs(referral_path, output_dir, batch_size):\n",
    "    \"\"\"\n",
    "    Batches the referral DataFrame by unique case_ref, preserving full notes.\n",
    "    \n",
    "    Args:\n",
    "        referral_path (str): Path to CSV file with referral data.\n",
    "        output_dir (str): Directory to save batch CSVs.\n",
    "        batch_size (int): Number of cases per batch.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of file paths to the saved batch CSVs.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(referral_path)\n",
    "    df = df[df[\"Referral Notes (depersonalised)\"].notnull()].reset_index(drop=True)\n",
    "    \n",
    "    # Keep only required columns for inference\n",
    "    df = df[[\"case_ref\", \"Referral Notes (depersonalised)\"]].copy()\n",
    "    df = df.rename(columns={\"Referral Notes (depersonalised)\": \"referral_note\"})\n",
    "\n",
    "    total = len(df)\n",
    "    num_batches = math.ceil(total / batch_size)\n",
    "\n",
    "    batch_paths = []\n",
    "    for i in range(num_batches):\n",
    "        batch_df = df.iloc[i*batch_size:(i+1)*batch_size]\n",
    "        batch_path = os.path.join(output_dir, f\"batch_{i:03d}.csv\")\n",
    "        batch_df.to_csv(batch_path, index=False)\n",
    "        batch_paths.append(batch_path)\n",
    "\n",
    "    return batch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "004ed6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_output_dir = os.path.join(\"../results/inference/full_referrals\", f\"test_{timestamp}\")\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "referral_path = \"../data/processed/brc-cleaned/toy_referrals.csv\"\n",
    "\n",
    "batch_paths = prepare_batched_csvs(referral_path, os.path.join(base_output_dir, \"batches\"), 5)\n",
    "\n",
    "test_batch_path = batch_paths[0]  # Use the first batch for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59218efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(base_output_dir, f\"predictions_batch_{0:03d}.csv\")\n",
    "\n",
    "# Load case-level batch\n",
    "df_batch = pd.read_csv(test_batch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d6a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for _, row in df_batch.iterrows():\n",
    "    case_ref = row[\"case_ref\"]\n",
    "    note = row[\"referral_note\"]\n",
    "    for sentence in sentence_splitter(note):\n",
    "        all_sentences.append({\n",
    "            \"case_ref\": case_ref,\n",
    "            \"Sentence\": sentence\n",
    "        })\n",
    "df_sentences = pd.DataFrame(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267832b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to temp file\n",
    "temp_path = os.path.join(base_output_dir, f\"temp_sentences_batch_{0:03d}.csv\")\n",
    "df_sentences.to_csv(temp_path, index=False)\n",
    "\n",
    "print(f\"âœ… Saved batched sentences to {temp_path}\")\n",
    "\n",
    "# Run two-step model\n",
    "run_two_step_pipeline(\n",
    "    data_file=temp_path,\n",
    "    roberta_model_dir=\"../results/model_training/roberta_binary/best_model/roberta-base_bs4_lr7e-05_20250726_140551/checkpoint-66\",\n",
    "    llama_model_dir=\"../results/model_training/llama_multilabel_direct_adverse/best_model/Llama-3.1-8B-Instruct_bs8_lr3e-05_epochs6_20250726_031804\",\n",
    "    pos_weight=1.1757,\n",
    "    output_file=out_path\n",
    ")\n",
    "\n",
    "os.remove(temp_path)  # clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f7aeccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Use nvtop Device 1 (A100)\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, Trainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scripts.roberta.dataset import BinarySDoHDataset, is_sdoh_label\n",
    "from scripts.roberta.model import RobertaBinaryClassifierWithWeight\n",
    "\n",
    "from scripts.llama.shared_utils.model import load_lora_llama\n",
    "from scripts.llama.multilabel_direct_adverse.prepare_dataset import prepare_adverse_only_dataset_infer, strip_protective_labels\n",
    "\n",
    "# Load test data\n",
    "df = pd.read_csv(temp_path)\n",
    "if \"completion\" in df.columns:\n",
    "    df[\"binary_label\"] = df[\"completion\"].apply(is_sdoh_label)\n",
    "\n",
    "# Load tokenizer and config from trained model directory\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "config = RobertaConfig.from_pretrained(\"../results/model_training/roberta_binary/best_model/roberta-base_bs4_lr7e-05_20250726_140551/checkpoint-66\")\n",
    "\n",
    "# Load model with pos_weight\n",
    "model = RobertaBinaryClassifierWithWeight.from_pretrained(\n",
    "    \"../results/model_training/roberta_binary/best_model/roberta-base_bs4_lr7e-05_20250726_140551/checkpoint-66\",\n",
    "    config=config,\n",
    "    pos_weight=1.1757\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc9cce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1908006/3709734502.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, tokenizer=tokenizer)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset\n",
    "dataset = BinarySDoHDataset(df, tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer)\n",
    "outputs = trainer.predict(dataset)\n",
    "\n",
    "# Get predictions\n",
    "probs = torch.sigmoid(torch.tensor(outputs.predictions)).numpy().flatten()\n",
    "y_pred = (probs > 0.4).astype(int) # Updated threshold\n",
    "\n",
    "# Add predictions to DataFrame\n",
    "df[\"roberta_prob_sdoh\"] = probs\n",
    "df[\"roberta_pred_sdoh\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4546d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32132294b147485b9bb68148e3e6d161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Loading LoRA adapters from: ../results/model_training/llama_multilabel_direct_adverse/best_model/Llama-3.1-8B-Instruct_bs8_lr3e-05_epochs6_20250726_031804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLaMA predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:09<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from scripts.multistep_adverse.two_step_pipeline import run_llama_on_flagged_sentences\n",
    "\n",
    "# Step 2: LLaMA\n",
    "llama_df = run_llama_on_flagged_sentences(\n",
    "    df_flagged=df[df[\"roberta_pred_sdoh\"] == 1],\n",
    "    model_dir=\"../results/model_training/llama_multilabel_direct_adverse/best_model/Llama-3.1-8B-Instruct_bs8_lr3e-05_epochs6_20250726_031804\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3953bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and fill\n",
    "# Merge and fill\n",
    "merge_cols = [\"Sentence\"]\n",
    "if \"case_ref\" in df.columns and \"case_ref\" in llama_df.columns:\n",
    "    merge_cols.insert(0, \"case_ref\")\n",
    "\n",
    "final_df = df.merge(llama_df, on=merge_cols, how=\"left\")\n",
    "final_df[\"final_prediction\"] = final_df.apply(\n",
    "    lambda row: row[\"generated_completion\"] if row[\"roberta_pred_sdoh\"] == 1 else \"<LIST>NoSDoH</LIST>\",\n",
    "    axis=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
