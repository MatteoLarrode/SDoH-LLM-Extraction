{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ceccc6",
   "metadata": {},
   "source": [
    "## Miscellaneous code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672a4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b6bc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cached models:\n",
      "  CohereForAI/aya-23-35B\n",
      "  CohereForAI/aya-23-8B\n",
      "  CohereForAI/aya-vision-8b\n",
      "  HuggingFaceTB/SmolLM-135M-Instruct\n",
      "  LLaMAX/LLaMAX3-8B-Alpaca\n",
      "  Qwen/Qwen1.5-4B\n",
      "  Qwen/Qwen2-7B\n",
      "  Qwen/Qwen2.5-1.5B\n",
      "  Qwen/Qwen2.5-3B\n",
      "  Qwen/Qwen2.5-72B-Instruct\n",
      "  Qwen/Qwen2.5-7B\n",
      "  Qwen/Qwen2.5-7B-Instruct\n",
      "  Qwen/Qwen2.5-7B-instruct\n",
      "  Qwen/Qwen2.5-VL-7B-Instruct\n",
      "  Qwen/Qwen3-0.6B\n",
      "  Qwen/Qwen3-8B\n",
      "  Unbabel/wmt20-comet-qe-da\n",
      "  Unbabel/wmt22-comet-da\n",
      "  bert-base-uncased\n",
      "  bert-large-uncased\n",
      "  cardiffnlp/twitter-roberta-base-sentiment\n",
      "  cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "  clairebarale/refugee_cases_ner\n",
      "  cross-encoder/nli-deberta-v3-large\n",
      "  cross-encoder/stsb-roberta-base\n",
      "  cross-encoder/stsb-roberta-large\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Llama-70B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "  facebook/nllb-200-3.3B\n",
      "  facebook/nllb-200-distilled-1.3B\n",
      "  facebook/nllb-200-distilled-600M\n",
      "  google/gemma-3-1b-it\n",
      "  google/gemma-3-27b-it\n",
      "  google/gemma-3-27b-it-qat-q4_0-gguf\n",
      "  gpt2\n",
      "  gpt2-medium\n",
      "  gpt2-xl\n",
      "  hfl/chinese-bert-wwm\n",
      "  hfl/chinese-electra-180g-small-discriminator\n",
      "  hfl/chinese-legal-electra-base-discriminator\n",
      "  hfl/chinese-legal-electra-small-discriminator\n",
      "  hfl/chinese-roberta-wwm-ext\n",
      "  hfl/chinese-roberta-wwm-ext-large\n",
      "  jxm/gtr__nq__32\n",
      "  jxm/gtr__nq__32__correct\n",
      "  meta-llama/Llama-2-7b-chat-hf\n",
      "  meta-llama/Llama-2-7b-hf\n",
      "  meta-llama/Llama-3.1-70B-Instruct\n",
      "  meta-llama/Llama-3.1-8B\n",
      "  meta-llama/Llama-3.1-8B-Instruct\n",
      "  meta-llama/Llama-3.2-11B-Vision-Instruct\n",
      "  meta-llama/Llama-3.3-70B-Instruct\n",
      "  meta-llama/Llama-4-Scout-17B-16E\n",
      "  meta-llama/Meta-Llama-3-70B-Instruct\n",
      "  meta-llama/Meta-Llama-3-8B\n",
      "  meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "  microsoft/Phi-3-mini-4k-instruct\n",
      "  microsoft/Phi-3.5-vision-instruct\n",
      "  microsoft/Phi-4-mini-instruct\n",
      "  microsoft/Phi-4-multimodal-instruct\n",
      "  microsoft/deberta-large-mnli\n",
      "  microsoft/deberta-v3-base\n",
      "  microsoft/deberta-xlarge\n",
      "  microsoft/deberta-xlarge-mnli\n",
      "  mistral-community/pixtral-12b\n",
      "  mistralai/Mistral-7B-Instruct-v0.2\n",
      "  mistralai/Mistral-7B-Instruct-v0.3\n",
      "  mistralai/Mistral-7B-v0.1\n",
      "  mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "  mosaicml/mpt-7b-chat\n",
      "  nlpaueb/legal-bert-base-uncased\n",
      "  nvidia/Llama-3.1-Nemotron-Nano-8B-v1\n",
      "  openai-community/gpt2\n",
      "  openai-community/gpt2-large\n",
      "  openai-community/gpt2-medium\n",
      "  openai-community/gpt2-xl\n",
      "  openai/whisper-large-v3-turbo\n",
      "  openbmb/MiniCPM-o-2_6\n",
      "  roberta-base\n",
      "  roberta-large\n",
      "  saibo/legal-roberta-base\n",
      "  sentence-transformers/LaBSE\n",
      "  sentence-transformers/all-MPNet-base-v2\n",
      "  sentence-transformers/all-MiniLM-L6-v2\n",
      "  sentence-transformers/all-mpnet-base-v2\n",
      "  sentence-transformers/gtr-t5-base\n",
      "  sentence-transformers/msmarco-bert-co-condensor\n",
      "  sentence-transformers/paraphrase-distilroberta-base-v2\n",
      "  sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "  sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "  shibing624/text2vec-base-chinese\n",
      "  t5-base\n",
      "  t5-large\n",
      "  t5-small\n",
      "  unsloth/llama-3-8b-bnb-4bit\n",
      "  unsloth/meta-llama-3.1-8b-instruct-bnb-4bit\n",
      "  unslothai/2\n",
      "  unslothai/4\n",
      "  unslothai/other\n",
      "  unslothai/repeat\n",
      "  unslothai/vram-48\n",
      "  vahidthegreat/StanceAware-SBERT\n",
      "  xlm-roberta-base\n",
      "  xlm-roberta-large\n"
     ]
    }
   ],
   "source": [
    "# Check models\n",
    "# What models are available\n",
    "cache_dir = \"/data/resource/huggingface/hub\"\n",
    "available_models = []\n",
    "\n",
    "if os.path.exists(cache_dir):\n",
    "    for item in os.listdir(cache_dir):\n",
    "        if item.startswith(\"models--\"):\n",
    "            # Convert models--org--name to org/name format\n",
    "            model_name = item.replace(\"models--\", \"\").replace(\"--\", \"/\")\n",
    "            available_models.append(model_name)\n",
    "\n",
    "print(\"Available cached models:\")\n",
    "for model in sorted(available_models):\n",
    "    print(f\"  {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e5ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  4 CUDA device(s) detected:\n",
      "\n",
      "Device 0: NVIDIA L40S\n",
      "Device 1: NVIDIA L40S\n",
      "Device 2: NVIDIA A100 80GB PCIe\n",
      "Device 3: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"ðŸ§  {torch.cuda.device_count()} CUDA device(s) detected:\\n\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1e087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.llama.multi_label_full.prepare_dataset import prepare_multilabel_dataset\n",
    "\n",
    "val_csv_prepared = prepare_multilabel_dataset(\"../data/processed/train-test/val_set.csv\")\n",
    "\n",
    "# Show first few examples\n",
    "val_csv_prepared_df = val_csv_prepared.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
