{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d71a71a",
   "metadata": {},
   "source": [
    "# SDoH Extraction using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f133f46",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51994ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb2a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbfe003",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b3ed5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cached models:\n",
      "  CohereForAI/aya-23-35B\n",
      "  CohereForAI/aya-23-8B\n",
      "  CohereForAI/aya-vision-8b\n",
      "  HuggingFaceTB/SmolLM-135M-Instruct\n",
      "  LLaMAX/LLaMAX3-8B-Alpaca\n",
      "  Qwen/Qwen2.5-1.5B\n",
      "  Qwen/Qwen2.5-3B\n",
      "  Qwen/Qwen2.5-72B-Instruct\n",
      "  Qwen/Qwen2.5-7B\n",
      "  Qwen/Qwen2.5-7B-Instruct\n",
      "  Qwen/Qwen2.5-7B-instruct\n",
      "  Qwen/Qwen2.5-VL-7B-Instruct\n",
      "  Unbabel/wmt20-comet-qe-da\n",
      "  Unbabel/wmt22-comet-da\n",
      "  bert-base-uncased\n",
      "  bert-large-uncased\n",
      "  cardiffnlp/twitter-roberta-base-sentiment\n",
      "  clairebarale/refugee_cases_ner\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Llama-70B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "  deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "  facebook/nllb-200-3.3B\n",
      "  facebook/nllb-200-distilled-1.3B\n",
      "  facebook/nllb-200-distilled-600M\n",
      "  gpt2\n",
      "  gpt2-medium\n",
      "  gpt2-xl\n",
      "  hfl/chinese-electra-180g-small-discriminator\n",
      "  hfl/chinese-legal-electra-base-discriminator\n",
      "  hfl/chinese-legal-electra-small-discriminator\n",
      "  hfl/chinese-roberta-wwm-ext\n",
      "  hfl/chinese-roberta-wwm-ext-large\n",
      "  jxm/gtr__nq__32\n",
      "  jxm/gtr__nq__32__correct\n",
      "  meta-llama/Llama-2-7b-chat-hf\n",
      "  meta-llama/Llama-2-7b-hf\n",
      "  meta-llama/Llama-3.1-70B-Instruct\n",
      "  meta-llama/Llama-3.1-8B\n",
      "  meta-llama/Llama-3.1-8B-Instruct\n",
      "  meta-llama/Llama-3.2-11B-Vision-Instruct\n",
      "  meta-llama/Llama-3.3-70B-Instruct\n",
      "  meta-llama/Llama-4-Scout-17B-16E\n",
      "  meta-llama/Meta-Llama-3-70B-Instruct\n",
      "  meta-llama/Meta-Llama-3-8B\n",
      "  meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "  microsoft/Phi-3-mini-4k-instruct\n",
      "  microsoft/Phi-3.5-vision-instruct\n",
      "  microsoft/Phi-4-mini-instruct\n",
      "  microsoft/Phi-4-multimodal-instruct\n",
      "  microsoft/deberta-v3-base\n",
      "  microsoft/deberta-xlarge\n",
      "  microsoft/deberta-xlarge-mnli\n",
      "  mistral-community/pixtral-12b\n",
      "  mistralai/Mistral-7B-Instruct-v0.2\n",
      "  mistralai/Mistral-7B-Instruct-v0.3\n",
      "  mistralai/Mistral-7B-v0.1\n",
      "  mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "  mosaicml/mpt-7b-chat\n",
      "  nlpaueb/legal-bert-base-uncased\n",
      "  nvidia/Llama-3.1-Nemotron-Nano-8B-v1\n",
      "  openai-community/gpt2\n",
      "  openbmb/MiniCPM-o-2_6\n",
      "  roberta-base\n",
      "  saibo/legal-roberta-base\n",
      "  sentence-transformers/LaBSE\n",
      "  sentence-transformers/all-MiniLM-L6-v2\n",
      "  sentence-transformers/all-mpnet-base-v2\n",
      "  sentence-transformers/gtr-t5-base\n",
      "  sentence-transformers/paraphrase-distilroberta-base-v2\n",
      "  sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
      "  t5-base\n",
      "  t5-large\n",
      "  t5-small\n",
      "  xlm-roberta-base\n",
      "  xlm-roberta-large\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Use shared cache\n",
    "os.environ['HF_HOME'] = '/data/resource/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/data/resource/huggingface/hub'\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'  # Force offline mode\n",
    "\n",
    "# What models are available\n",
    "cache_dir = \"/data/resource/huggingface/hub\"\n",
    "available_models = []\n",
    "\n",
    "if os.path.exists(cache_dir):\n",
    "    for item in os.listdir(cache_dir):\n",
    "        if item.startswith(\"models--\"):\n",
    "            # Convert models--org--name to org/name format\n",
    "            model_name = item.replace(\"models--\", \"\").replace(\"--\", \"/\")\n",
    "            available_models.append(model_name)\n",
    "\n",
    "print(\"Available cached models:\")\n",
    "for model in sorted(available_models):\n",
    "    print(f\"  {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c75c022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.3\n",
      "PyTorch version: 2.6.0\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423c8c5",
   "metadata": {},
   "source": [
    "## Prototype zero-shot extraction of SDoH from a note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9487dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned data\n",
    "brc_referrals_cleaned = pd.read_csv(\"../data/processed/BRC_referrals_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f573d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Note:\n",
      "Befriending. Lives with husband for whom patient is carer. Living on ready meals at present. PERSON concerned that they may not be eating / drinking enough. Carers in XXXX times daily for patient to help with washing / dressing. Depending on side - effects of radiotherapy , patient may go on to PEG feeding. Patient feeling slightly overwhelmed by everything. FPOC and Carers Support Shropshire numbers given to patient â€™s daughter ( with patient consent ) but would value additional emotional support / befriending / check ins to make sure they are eating / drinking etc as all activities of daily living severely compromised at present. Very supportive daughter who lives in PERSON. The patient is due to start radiotherapy on XXXX January so support during this time would be particularly beneficial. Due to start radiotherapy on XXXX at SATH EOS XXXX.\n"
     ]
    }
   ],
   "source": [
    "# Load a specific note: Case Reference = CAS-467812\n",
    "note = brc_referrals_cleaned[brc_referrals_cleaned['Case Reference'] == 'CAS-467812'].iloc[0]['Referral Notes (depersonalised)']\n",
    "\n",
    "print(\"Sample Note:\")\n",
    "print(note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a976f",
   "metadata": {},
   "source": [
    "### Defining the classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3309a",
   "metadata": {},
   "source": [
    "For now, I define the same classification task as Guevara et al. (2024):\n",
    "\n",
    "- **Task**: Multi-label sentence-level classification\n",
    "- **SDoH**: 6 SDoH categories\n",
    "    - Employment status\n",
    "    - Housing issues\n",
    "    - Transportation issues\n",
    "    - Parental status\n",
    "    - Relationship status\n",
    "    - Social support\n",
    "\n",
    "I define two levels of classification:\n",
    "\n",
    "1. Level 1: *Any SDoH mentions*. The presence of language describing an SDoH category as defined above, regardless of the attribute. \n",
    "2. Level 2: *Adverse SDoH mentions*. The presence or absence of language describing an SDoH category with an attribute that could create an additional social work or resource support need for patients:\n",
    "    - Employment status: unemployed, underemployed, disability\n",
    "    - Housing issue: financial status, undomiciled, other\n",
    "    - Transportation issue: distance, resources, other\n",
    "    - Parental status: having a child under 18 years old\n",
    "    - Relationship: widowed, divorced, single\n",
    "    - Social support: absence of social support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08e00b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "from utils.model_helpers import *\n",
    "from utils.classification_helpers import *\n",
    "\n",
    "\n",
    "# Load one of the instruction-tuned models\n",
    "# Qwen/Qwen2.5-7B-Instruct - Excellent for zero-shot classification\n",
    "# meta-llama/Llama-3.1-8B-Instruct - Very capable instruction-following model\n",
    "# microsoft/Phi-4-mini-instruct - Smaller but efficient\n",
    "# mistralai/Mistral-7B-Instruct-v0.3 - Great performance on classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer, model = None, None\n",
    "\n",
    "print(f\"Trying to load {model_name}...\")\n",
    "tokenizer, model = load_instruction_model(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
