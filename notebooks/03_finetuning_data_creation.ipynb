{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e413d4d",
   "metadata": {},
   "source": [
    "# Curation and creation of data for LLM finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cf3e",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c7d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38c32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd8299",
   "metadata": {},
   "source": [
    "## 1. Annotation train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a710356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.annotation.parse_annotations_helpers import parse_labelstudio_json\n",
    "round1_labelstudio_path = \"../data/processed/annotations/label-studio/label-studio-annotations-2025-07-24-15-40-21147c14.json\"\n",
    "\n",
    "# === Step 0: Clean and group ===\n",
    "df = parse_labelstudio_json(round1_labelstudio_path)\n",
    "df = df[df[\"SDoH\"].notnull()]\n",
    "df[\"SDoH\"] = df[\"SDoH\"].str.strip()\n",
    "df[\"Polarity\"] = df[\"Polarity\"].fillna(\"\").str.strip()\n",
    "\n",
    "# === Define canonical label mapping (annotation → model expected) ===\n",
    "label_name_map = {\n",
    "    \"Food\": \"FoodAccess\",\n",
    "    \"English\": \"EnglishProficiency\",\n",
    "    \"No SDoH\": \"NoSDoH\",  # just to catch if it sneaks in\n",
    "}\n",
    "\n",
    "def make_label(sdoh, polarity):\n",
    "    sdoh = sdoh.strip()\n",
    "    polarity = polarity.strip()\n",
    "    \n",
    "    # Normalize label name\n",
    "    sdoh_key = sdoh.lower().replace(\" \", \"\")\n",
    "    if sdoh_key == \"nosdoh\":\n",
    "        return \"NoSDoH\"\n",
    "    \n",
    "    # Map to canonical names\n",
    "    sdoh = label_name_map.get(sdoh, sdoh)\n",
    "\n",
    "    return f\"{sdoh}-{polarity}\"\n",
    "\n",
    "df[\"label_pair\"] = df.apply(lambda row: make_label(row[\"SDoH\"], row[\"Polarity\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19040f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 520, Val size: 130, Test size: 278\n",
      "Included 16 singleton label combinations split in two steps (80/20 then 80/20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Group per sentence\n",
    "sentence_labels = (\n",
    "    df.groupby(\"Sentence\")[\"label_pair\"]\n",
    "    .apply(lambda x: sorted(set(x)))\n",
    "    .reset_index()\n",
    ")\n",
    "sentence_labels[\"label_string\"] = sentence_labels[\"label_pair\"].apply(lambda x: \"|\".join(x))\n",
    "\n",
    "# === Step 1: Separate singleton label groups ===\n",
    "label_counts = sentence_labels[\"label_string\"].value_counts()\n",
    "singleton_labels = label_counts[label_counts == 1].index\n",
    "\n",
    "non_singletons = sentence_labels[sentence_labels[\"label_string\"].isin(label_counts[label_counts > 1].index)]\n",
    "singletons = sentence_labels[sentence_labels[\"label_string\"].isin(singleton_labels)]\n",
    "\n",
    "# === Step 2: First split: train/test (70/30) for non-singletons ===\n",
    "train_ns, test_ns = train_test_split(\n",
    "    non_singletons,\n",
    "    test_size=0.3,\n",
    "    stratify=non_singletons[\"label_string\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# === Step 3: Second split: train/val (80/20 of training set) for non-singletons ===\n",
    "train_ns, val_ns = train_test_split(\n",
    "    train_ns,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True  # still shuffles\n",
    ")\n",
    "\n",
    "# === Step 4: Manual split for singletons (70/30 → then 80/20 again) ===\n",
    "np.random.seed(42)\n",
    "singleton_shuffled = singletons.sample(frac=1.0, random_state=42)\n",
    "\n",
    "n_singleton_total = len(singleton_shuffled)\n",
    "n_test = int(n_singleton_total * 0.3)\n",
    "n_remaining = n_singleton_total - n_test\n",
    "n_val = int(n_remaining * 0.2)\n",
    "n_train = n_remaining - n_val\n",
    "\n",
    "test_singletons = singleton_shuffled.iloc[:n_test]\n",
    "val_singletons = singleton_shuffled.iloc[n_test:n_test + n_val]\n",
    "train_singletons = singleton_shuffled.iloc[n_test + n_val:]\n",
    "\n",
    "# === Step 5: Combine splits ===\n",
    "train_set = pd.concat([train_ns, train_singletons], ignore_index=True)\n",
    "val_set   = pd.concat([val_ns, val_singletons], ignore_index=True)\n",
    "test_set  = pd.concat([test_ns, test_singletons], ignore_index=True)\n",
    "\n",
    "# === Step 6: Format completions ===\n",
    "for df_ in [train_set, val_set, test_set]:\n",
    "    df_[\"completion\"] = df_[\"label_pair\"].apply(lambda x: \"<LIST>\" + \", \".join(x) + \"</LIST>\")\n",
    "\n",
    "# === Step 7: Save splits ===\n",
    "train_set.to_csv(\"../data/processed/train-test/train_set.csv\", index=False)\n",
    "val_set.to_csv(\"../data/processed/train-test/val_set.csv\", index=False)\n",
    "test_set.to_csv(\"../data/processed/train-test/test_set.csv\", index=False)\n",
    "\n",
    "print(f\"Train size: {len(train_set)}, Val size: {len(val_set)}, Test size: {len(test_set)}\")\n",
    "print(f\"Included {len(singletons)} singleton label combinations split in two steps (80/20 then 80/20)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc09a1",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7beadd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Dataset    | % NoSDoH    | % Finances   | % FoodAccess   | % Employment   | % Loneliness   | % Housing   | % DigitalInclusion   | % EnglishProficiency   |\n",
      "|:-----------|:------------|:-------------|:---------------|:---------------|:---------------|:------------|:---------------------|:-----------------------|\n",
      "| Train      | 54.0% (281) | 7.5% (39)    | 9.4% (49)      | 5.0% (26)      | 17.7% (92)     | 11.9% (62)  | 4.6% (24)            | 4.8% (25)              |\n",
      "| Validation | 53.8% (70)  | 9.2% (12)    | 10.0% (13)     | 5.4% (7)       | 20.8% (27)     | 11.5% (15)  | 2.3% (3)             | 1.5% (2)               |\n",
      "| Test       | 54.0% (150) | 6.8% (19)    | 9.0% (25)      | 5.0% (14)      | 18.0% (50)     | 12.2% (34)  | 4.0% (11)            | 4.7% (13)              |\n",
      "| Combined   | 54.0% (501) | 7.5% (70)    | 9.4% (87)      | 5.1% (47)      | 18.2% (169)    | 12.0% (111) | 4.1% (38)            | 4.3% (40)              |\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def compute_sdoh_proportions_with_counts(df, dataset_name):\n",
    "    df = df.copy()\n",
    "    df[\"label_pair\"] = df[\"label_pair\"].apply(ast.literal_eval)\n",
    "    df[\"flattened\"] = df[\"label_pair\"].apply(set)\n",
    "\n",
    "    df[\"has_NoSDoH\"] = df[\"flattened\"].apply(lambda s: s == {\"NoSDoH\"})\n",
    "\n",
    "    sdoh_categories = [\n",
    "        \"Finances\", \"FoodAccess\", \"Employment\", \"Loneliness\",\n",
    "        \"Housing\", \"DigitalInclusion\", \"EnglishProficiency\"\n",
    "    ]\n",
    "\n",
    "    for cat in sdoh_categories:\n",
    "        df[cat] = df[\"flattened\"].apply(lambda labels: any(label.startswith(cat) for label in labels))\n",
    "\n",
    "    total = len(df)\n",
    "    row = {\"Dataset\": dataset_name}\n",
    "    row[\"% NoSDoH\"] = f\"{df['has_NoSDoH'].mean() * 100:.1f}% ({df['has_NoSDoH'].sum()})\"\n",
    "    for cat in sdoh_categories:\n",
    "        count = df[cat].sum()\n",
    "        perc = (count / total) * 100\n",
    "        row[f\"% {cat}\"] = f\"{perc:.1f}% ({count})\"\n",
    "    return pd.DataFrame([row])\n",
    "\n",
    "# === Load CSVs ===\n",
    "train_df = pd.read_csv(\"../data/processed/train-test/train_set.csv\")\n",
    "val_df = pd.read_csv(\"../data/processed/train-test/val_set.csv\")\n",
    "test_df = pd.read_csv(\"../data/processed/train-test/test_set.csv\")\n",
    "\n",
    "# === Compute table ===\n",
    "train_stats = compute_sdoh_proportions_with_counts(train_df, \"Train\")\n",
    "val_stats = compute_sdoh_proportions_with_counts(val_df, \"Validation\")\n",
    "test_stats = compute_sdoh_proportions_with_counts(test_df, \"Test\")\n",
    "combined_stats = compute_sdoh_proportions_with_counts(\n",
    "    pd.concat([train_df, val_df, test_df], ignore_index=True), \"Combined\"\n",
    ")\n",
    "\n",
    "proportions_table = pd.concat([train_stats, val_stats, test_stats, combined_stats], ignore_index=True)\n",
    "print(proportions_table.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
