{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e413d4d",
   "metadata": {},
   "source": [
    "# Curation and creation of data for LLM finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cf3e",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c7d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c38c32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Add the project root to the Python path to import the modules\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd8299",
   "metadata": {},
   "source": [
    "## 1. Annotation train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19040f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 564, Test size: 243\n",
      "Included 11 singleton label combinations (distributed manually)\n"
     ]
    }
   ],
   "source": [
    "from src.annotation.parse_annotations_helpers import parse_labelstudio_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "round1_labelstudio_path = \"../data/processed/annotations/label-studio/label-studio-annotations-2025-06-29-17-28-05ccd4c1.json\"\n",
    "\n",
    "# === Step 0: Clean and group as before ===\n",
    "df = parse_labelstudio_json(round1_labelstudio_path)\n",
    "df = df[df[\"SDoH\"].notnull()]\n",
    "df[\"SDoH\"] = df[\"SDoH\"].str.strip()\n",
    "df[\"Polarity\"] = df[\"Polarity\"].fillna(\"\").str.strip()\n",
    "\n",
    "def make_label(sdoh, polarity):\n",
    "    sdoh = sdoh.strip()\n",
    "    polarity = polarity.strip()\n",
    "    \n",
    "    # Normalize space for \"No SDoH\"\n",
    "    if sdoh.lower().replace(\" \", \"\") == \"nosdoh\":\n",
    "        return \"NoSDoH\"\n",
    "    else:\n",
    "        return f\"{sdoh}-{polarity}\"\n",
    "\n",
    "df[\"label_pair\"] = df.apply(lambda row: make_label(row[\"SDoH\"], row[\"Polarity\"]), axis=1)\n",
    "\n",
    "# Group per sentence\n",
    "sentence_labels = (\n",
    "    df.groupby(\"Sentence\")[\"label_pair\"]\n",
    "    .apply(lambda x: sorted(set(x)))\n",
    "    .reset_index()\n",
    ")\n",
    "sentence_labels[\"label_string\"] = sentence_labels[\"label_pair\"].apply(lambda x: \"|\".join(x))\n",
    "\n",
    "# === Step 1: Separate singleton label groups ===\n",
    "label_counts = sentence_labels[\"label_string\"].value_counts()\n",
    "singleton_labels = label_counts[label_counts == 1].index\n",
    "\n",
    "# Split into regular and singleton groups\n",
    "non_singletons = sentence_labels[sentence_labels[\"label_string\"].isin(label_counts[label_counts > 1].index)]\n",
    "singletons = sentence_labels[sentence_labels[\"label_string\"].isin(singleton_labels)]\n",
    "\n",
    "# === Step 2: Stratified split for non-singletons ===\n",
    "train_ns, test_ns = train_test_split(\n",
    "    non_singletons,\n",
    "    test_size=0.3,\n",
    "    stratify=non_singletons[\"label_string\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# === Step 3: Manual split for singletons (e.g. 70/30) ===\n",
    "np.random.seed(42)\n",
    "singleton_shuffled = singletons.sample(frac=1.0, random_state=42)\n",
    "n_singleton_train = int(len(singleton_shuffled) * 0.7)\n",
    "\n",
    "train_singletons = singleton_shuffled.iloc[:n_singleton_train]\n",
    "test_singletons = singleton_shuffled.iloc[n_singleton_train:]\n",
    "\n",
    "# === Step 4: Combine final splits ===\n",
    "train_set = pd.concat([train_ns, train_singletons], ignore_index=True)\n",
    "test_set = pd.concat([test_ns, test_singletons], ignore_index=True)\n",
    "\n",
    "# === Step 5: Format completions for prompting or fine-tuning ===\n",
    "train_set[\"completion\"] = train_set[\"label_pair\"].apply(lambda x: \"<LIST>\" + \", \".join(x) + \"</LIST>\")\n",
    "test_set[\"completion\"] = test_set[\"label_pair\"].apply(lambda x: \"<LIST>\" + \", \".join(x) + \"</LIST>\")\n",
    "\n",
    "# Save the train and test sets\n",
    "train_set.to_csv(\"../data/processed/train-test/train_set.csv\", index=False)\n",
    "test_set.to_csv(\"../data/processed/train-test/test_set.csv\", index=False)\n",
    "\n",
    "print(f\"Train size: {len(train_set)}, Test size: {len(test_set)}\")\n",
    "print(f\"Included {len(singletons)} singleton label combinations (distributed manually)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01cd67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a537956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da1890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beec2a59",
   "metadata": {},
   "source": [
    "## 2. Curation of manually annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classification.prompt_creation_helpers import create_automated_prompt\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# === Step 2: Group SDoH-Polarity pairs per sentence ===\n",
    "def format_label(sdoh, polarity):\n",
    "    return f\"{sdoh.strip()}-{polarity.strip()}\"\n",
    "\n",
    "df[\"label_pair\"] = df.apply(lambda row: format_label(row[\"SDoH\"], row[\"Polarity\"]), axis=1)\n",
    "\n",
    "grouped = (\n",
    "    df.groupby(\"Sentence\")[\"label_pair\"]\n",
    "    .apply(lambda labels: \"<LIST>\" + \", \".join(sorted(set(labels))) + \"</LIST>\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"label_pair\": \"completion\"})\n",
    ")\n",
    "\n",
    "# === Step 3: Generate prompts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "grouped[\"prompt\"] = grouped[\"Sentence\"].apply(\n",
    "    lambda s: create_automated_prompt(s, tokenizer=tokenizer, prompt_type=\"five_shot_basic\")\n",
    ")\n",
    "\n",
    "# === Step 4: Final dataset\n",
    "finetune_dataset = Dataset.from_pandas(grouped[[\"prompt\", \"completion\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa5b462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are analyzing a referral note sentence to identify Social Determinants of Health, and classifying them as Adverse or Protective.\n",
      "\n",
      "Given a sentence, output all SDoH factors that can be inferred from that sentence from the following list: \n",
      "Loneliness, Housing, Finances, FoodAccess, Digital, Employment, EnglishProficiency.\n",
      "\n",
      "Each SDoH must be classified as either \"Adverse\" or \"Protective\". \n",
      "If the sentence does NOT mention any of the above categories, output <LIST>NoSDoH</LIST>.\n",
      "\n",
      "Your response must be a comma-separated list of SDoH-Polarity pairs embedded in <LIST> and </LIST> tags.\n",
      "\n",
      "**STRICT RULES**:\n",
      "- DO NOT generate any other text, explanations, or new SDoH labels.\n",
      "- A sentence CAN be labeled with one or more SDoH factors.\n",
      "- The only accepted format is <LIST>...</LIST>.\n",
      "\n",
      "EXAMPLES:\n",
      "Input: \"She is unemployed and struggles to pay rent.\"\n",
      "Output: <LIST>Employment-Adverse, Finances-Adverse, Housing-Adverse</LIST>\n",
      "\n",
      "Input: \"We are referring the above patient to you today for befriending.\"\n",
      "Output: <LIST>Loneliness-Adverse</LIST>\n",
      "\n",
      "Input: \"She enjoys a strong network of friends and volunteers weekly.\"\n",
      "Output: <LIST>Loneliness-Protective</LIST>\n",
      "\n",
      "Input: \"Sleeping at a friend's for now.\"\n",
      "Output: <LIST>Housing-Adverse</LIST>\n",
      "\n",
      "Input: \"Cannot take public transport to do groceries.\"\n",
      "Output: <LIST>FoodAccess-Adverse</LIST>\n",
      "\n",
      "Input: \"Daughter translates at GP visits.\"\n",
      "Output: <LIST>EnglishProficiency-Adverse</LIST><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Input: \"-Befriending Visits * Fall with head injury * Lives alone with no family or close friends , reports feeling lonely and would like some company to interact with .\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<LIST>Loneliness-Adverse</LIST>\n"
     ]
    }
   ],
   "source": [
    "# Output example\n",
    "print(finetune_dataset[0][\"prompt\"])\n",
    "print(finetune_dataset[0][\"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d13b59",
   "metadata": {},
   "source": [
    "## 3. Creation of synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36fcd8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keble8263-sdoh-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
